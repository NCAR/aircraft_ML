{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViqmHWqfGumf"
      },
      "source": [
        "# Particle Classification CNN: 4-Class Hybrid Model\n",
        "\n",
        "This notebook trains a CNN to classify cloud particles into 4 phases using:\n",
        "- Particle images (128x128 grayscale)\n",
        "- Environmental features: temperature, air speed, altitude\n",
        "\n",
        "**Output:** 4-class classification\n",
        "- Phase 0: Liquid\n",
        "- Phase 1: Solid (Ice)\n",
        "- Phase 2: Donut\n",
        "- Phase 3: Noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "morPzRTFGumg"
      },
      "source": [
        "## 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQQZjoPzGumg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Concatenate, BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rATtmL5Gumg"
      },
      "source": [
        "## 2. Load and Prepare Data\n",
        "\n",
        "**NOTE:** You need to have a DataFrame with the following columns:\n",
        "- `particle_idx_seq`: matches the particle_X.png filenames\n",
        "- `phase`: 0 for liquid, 1 for solid, 2 for donut, 3 for noise\n",
        "- `ATX`: temperature value\n",
        "- `TASX`: air speed value\n",
        "- `GGALT`: altitude value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuRXs8OcGumg",
        "outputId": "73a1cfab-12b8-4034-e24b-b3f765ccfb04"
      },
      "outputs": [],
      "source": [
        "#connect to google drive\n",
        "from google.colab import drive\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "_Ky1ERnbGumh",
        "outputId": "9bfb77f7-be33-4ac1-9d75-614ff40a9e1d"
      },
      "outputs": [],
      "source": [
        "base_path = '/content/drive/MyDrive/GEOG5100/aircraft_ml/'\n",
        "\n",
        "# Load labeled data\n",
        "df = pd.read_csv(base_path+'particle_df.csv')\n",
        "if df is None:\n",
        "    raise ValueError(\"Please load your labeled dataframe before proceeding\")\n",
        "\n",
        "# Display basic info about the dataset\n",
        "print(f\"Total labeled particles: {len(df)}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(df['phase'].value_counts().sort_index())\n",
        "print(f\"\\nDataFrame info:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vdjCkpvGumh"
      },
      "source": [
        "## 3. Load Particle Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "511JMsMzGumh"
      },
      "outputs": [],
      "source": [
        "def load_particle_image(particle_num, image_dir=base_path, target_size=(128, 128)):\n",
        "    \"\"\"\n",
        "    Load a single particle image and preprocess it.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    particle_num : int\n",
        "        Particle number (corresponds to particle_X.png)\n",
        "    image_dir : str\n",
        "        Directory containing particle images\n",
        "    target_size : tuple\n",
        "        Target image size (height, width)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    image : np.ndarray\n",
        "        Normalized image array of shape (target_size[0], target_size[1], 1)\n",
        "    \"\"\"\n",
        "    # Check all possible directories in order\n",
        "    subdirs = ['liquid', 'solid']\n",
        "\n",
        "    for subdir in subdirs:\n",
        "        img_path = os.path.join(image_dir, f'particle_images_filtered/{subdir}', f'particle_{particle_num}.png')\n",
        "        if os.path.exists(img_path):\n",
        "            # Load image as grayscale\n",
        "            img = Image.open(img_path).convert('L')\n",
        "\n",
        "            # Resize if necessary\n",
        "            img = img.resize(target_size)\n",
        "\n",
        "            # Convert to numpy array and normalize to [0, 1]\n",
        "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
        "\n",
        "            # Add channel dimension\n",
        "            img_array = np.expand_dims(img_array, axis=-1)\n",
        "\n",
        "            return img_array\n",
        "\n",
        "    #print(f\"Warning: Image not found for particle {particle_num}\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xqW0KjfGumh",
        "outputId": "fca92e48-9baf-447f-bcdb-aeca83d3ab00"
      },
      "outputs": [],
      "source": [
        "# Load all images (with parallel processing for speed)\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Loading particle images in parallel...\")\n",
        "\n",
        "def load_image_wrapper(args):\n",
        "    \"\"\"Wrapper function for parallel image loading\"\"\"\n",
        "    idx, particle_num = args\n",
        "    img = load_particle_image(particle_num)\n",
        "    return idx, img\n",
        "\n",
        "# Prepare arguments for parallel processing\n",
        "load_args = [(idx, row['particle_idx_seq'])\n",
        "             for idx, row in df.iterrows()]\n",
        "\n",
        "# Load images in parallel using ThreadPoolExecutor\n",
        "images = []\n",
        "valid_indices = []\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "    # Use tqdm for progress bar\n",
        "    results = list(tqdm(executor.map(load_image_wrapper, load_args),\n",
        "                        total=len(load_args),\n",
        "                        desc=\"Loading images\"))\n",
        "\n",
        "    for idx, img in results:\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "            valid_indices.append(idx)\n",
        "\n",
        "# Convert to numpy array\n",
        "X_images = np.array(images)\n",
        "\n",
        "# Filter dataframe to only include particles with valid images\n",
        "df_valid = df.loc[valid_indices].reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nSuccessfully loaded {len(X_images)} images\")\n",
        "print(f\"Image shape: {X_images.shape}\")\n",
        "\n",
        "# Filter out phases 2 and 3 from both df_valid and X_images to ensure alignment\n",
        "initial_num_images = len(X_images)\n",
        "initial_num_df_rows = len(df_valid)\n",
        "\n",
        "phase_filter_mask = df_valid['phase'].isin([0, 1])\n",
        "\n",
        "df_valid = df_valid[phase_filter_mask].reset_index(drop=True)\n",
        "X_images = X_images[phase_filter_mask]\n",
        "\n",
        "print(f\"Filtered for phases 0 and 1. Dropped {initial_num_df_rows - len(df_valid)} rows.\")\n",
        "print(f\"New X_images shape: {X_images.shape}\")\n",
        "print(f\"New df_valid shape: {df_valid.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhXYXqO2Gumh"
      },
      "source": [
        "## 4. Prepare Environmental Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jbWe41sGumh",
        "outputId": "4d2cd04d-db69-4d79-f476-c402d5e33d1e"
      },
      "outputs": [],
      "source": [
        "# Extract environmental features\n",
        "print(\"Extracting feature names\")\n",
        "feature_columns = ['area','xsize']\n",
        "X_features = df_valid[feature_columns].values\n",
        "\n",
        "# Standardize the features (important for neural networks)\n",
        "scaler = StandardScaler()\n",
        "X_features_scaled = scaler.fit_transform(X_features)\n",
        "\n",
        "# Handle NaN values in scaled features, replacing them with 0.0 (the mean of scaled features)\n",
        "X_features_scaled = np.nan_to_num(X_features_scaled, nan=0.0)\n",
        "\n",
        "print(f\"Environmental features shape: {X_features_scaled.shape}\")\n",
        "print(f\"\\nFeature statistics (after scaling and NaN handling):\")\n",
        "print(pd.DataFrame(X_features_scaled, columns=feature_columns).describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkkGnBCQpcbG",
        "outputId": "5dee90e0-2c31-44cd-8da3-c19e5a973a55"
      },
      "outputs": [],
      "source": [
        "print(\"Preparing data file paths and features...\")\n",
        "\n",
        "# Add the full image path to the DataFrame\n",
        "def get_full_image_path(row, image_dir=base_path):\n",
        "    \"\"\"Determine the full path for a particle image.\"\"\"\n",
        "    particle_num = row['particle_idx_seq']\n",
        "    subdirs = ['liquid', 'solid']\n",
        "\n",
        "    for subdir in subdirs:\n",
        "        img_path = os.path.join(image_dir, f'particle_images_filtered/{subdir}', f'particle_{particle_num}.png')\n",
        "        if os.path.exists(img_path):\n",
        "            return img_path\n",
        "    return None\n",
        "\n",
        "# Find all paths and filter the DataFrame\n",
        "df['image_path'] = df.apply(get_full_image_path, axis=1)\n",
        "df_valid = df.dropna(subset=['image_path']).reset_index(drop=True)\n",
        "\n",
        "print(f\"Total valid samples with images found: {len(df_valid)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lgkdfbf5poSX",
        "outputId": "8e478c34-eda9-495d-9799-493569a2fa6b"
      },
      "outputs": [],
      "source": [
        "# --- Step 2: Filter for specific phases (0 and 1) and synchronize data ---\n",
        "\n",
        "# Filter out phases 2 and 3\n",
        "phase_filter_mask = df_valid['phase'].isin([0, 1])\n",
        "df_valid = df_valid[phase_filter_mask].reset_index(drop=True)\n",
        "\n",
        "print(f\"Filtered for phases 0 and 1. Final samples: {len(df_valid)}\")\n",
        "\n",
        "# Extract the final synchronized arrays\n",
        "image_paths = df_valid['image_path'].values\n",
        "#Use x_features that we scaled\n",
        "#X_features # = df_valid[feature_columns].values # Assuming feature_columns is a list of column names\n",
        "y_labels = pd.get_dummies(df_valid['phase']).values # Convert phase (0, 1) to one-hot labels (categorical)\n",
        "\n",
        "print(f\"Synchronized Data Shapes:\")\n",
        "print(f\"  Image Paths: {image_paths.shape}\")\n",
        "print(f\"  Features: {X_features_scaled.shape}\")\n",
        "print(f\"  Labels (Categorical): {y_labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1719OPdZpqpq",
        "outputId": "32b0f395-9217-4084-bafc-ddcce160684f"
      },
      "outputs": [],
      "source": [
        "# --- Step 3: Data Splitting (Image Paths, Features, Labels) ---\n",
        "\n",
        "print(\"\\nSplitting data...\")\n",
        "# Initial split: 90% temp, 10% test\n",
        "X_paths_temp, X_paths_test, \\\n",
        "X_features_temp, X_features_test, \\\n",
        "y_temp, y_test = train_test_split(\n",
        "    image_paths, X_features_scaled, y_labels,\n",
        "    test_size=0.10,\n",
        "    stratify=y_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Second split: 89% train, 11% validation (of temp)\n",
        "X_paths_train, X_paths_val, \\\n",
        "X_features_train, X_features_val, \\\n",
        "y_train, y_val = train_test_split(\n",
        "    X_paths_temp, X_features_temp, y_temp,\n",
        "    test_size=0.11,\n",
        "    stratify=y_temp,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set: {len(X_paths_train)} samples\")\n",
        "print(f\"Validation set: {len(X_paths_val)} samples\")\n",
        "print(f\"Test set: {len(X_paths_test)} samples\")\n",
        "print(\"Data is ready for the tf.data.Dataset pipeline.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAxs_eVzGumh",
        "outputId": "5026633b-9237-487d-e3ae-9d88543b9cad"
      },
      "outputs": [],
      "source": [
        "# Extract labels and convert to one-hot encoding\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "y = df_valid['phase'].values\n",
        "y_categorical = to_categorical(y, num_classes=2)\n",
        "\n",
        "print(f\"Labels shape: {y_categorical.shape}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "print(f\"Class 0 (Liquid): {np.sum(y == 0)} samples ({np.sum(y == 0) / len(y) * 100:.1f}%)\")\n",
        "print(f\"Class 1 (Solid): {np.sum(y == 1)} samples ({np.sum(y == 1) / len(y) * 100:.1f}%)\")\n",
        "# print(f\"Class 2 (Donut): {np.sum(y == 2)} samples ({np.sum(y == 2) / len(y) * 100:.1f}%)\")\n",
        "# print(f\"Class 3 (Noise): {np.sum(y == 3)} samples ({np.sum(y == 3) / len(y) * 100:.1f}%)\")\n",
        "\n",
        "# Calculate class weights for imbalanced data\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y),\n",
        "    y=y\n",
        ")\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "print(f\"\\nClass weights (to handle imbalance):\")\n",
        "for i, w in class_weight_dict.items():\n",
        "    print(f\"  Class {i}: {w:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH51YK5dqRfW"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14cCGiMxaSnc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from typing import Tuple\n",
        "\n",
        "IMAGE_SIZE = (128, 128)\n",
        "\n",
        "def random_geometric_augment(image: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"\n",
        "    Apply geometric augmentations suitable for binary particle images.\n",
        "    \"\"\"\n",
        "    # Random 90-degree rotations (0, 90, 180, or 270 degrees)\n",
        "    if tf.random.uniform(()) < 0.5:\n",
        "        k = tf.random.uniform((), minval=0, maxval=4, dtype=tf.int32)\n",
        "        image = tf.image.rot90(image, k=k)\n",
        "\n",
        "    # Random horizontal flip\n",
        "    if tf.random.uniform(()) < 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "\n",
        "    # Random vertical flip\n",
        "    if tf.random.uniform(()) < 0.5:\n",
        "        image = tf.image.flip_up_down(image)\n",
        "\n",
        "    # Random zoom (via crop and resize) - equivalent to zoom_range=0.2\n",
        "    if tf.random.uniform(()) < 0.5:\n",
        "        # Zoom range of 0.2 means 80% to 100% crop\n",
        "        crop_factor = tf.random.uniform((), 0.8, 1.0)\n",
        "        crop_size = tf.cast(IMAGE_SIZE[0] * crop_factor, tf.int32)\n",
        "        image = tf.image.random_crop(image, size=[crop_size, crop_size, 1])\n",
        "        image = tf.image.resize(image, IMAGE_SIZE)\n",
        "\n",
        "    # Random translation (width_shift and height_shift of 0.1)\n",
        "    if tf.random.uniform(()) < 0.5:\n",
        "        # 10% of image size = 12.8 pixels, round to 13\n",
        "        max_shift = int(IMAGE_SIZE[0] * 0.1)\n",
        "        shift_height = tf.random.uniform((), -max_shift, max_shift, dtype=tf.int32)\n",
        "        shift_width = tf.random.uniform((), -max_shift, max_shift, dtype=tf.int32)\n",
        "\n",
        "        # Pad image to allow for translation\n",
        "        padded = tf.pad(image, [[max_shift, max_shift], [max_shift, max_shift], [0, 0]],\n",
        "                       constant_values=0.0)\n",
        "\n",
        "        # Crop to simulate translation\n",
        "        offset_height = max_shift + shift_height\n",
        "        offset_width = max_shift + shift_width\n",
        "        image = tf.image.crop_to_bounding_box(padded, offset_height, offset_width,\n",
        "                                              IMAGE_SIZE[0], IMAGE_SIZE[1])\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8yzM1O_yrVo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from typing import Tuple\n",
        "\n",
        "# Define the image size based on your model architecture\n",
        "IMAGE_SIZE = (128, 128)\n",
        "AUGMENTATION_PROB = 0.5\n",
        "MAX_SHIFT = 0.2 # 10% shift\n",
        "\n",
        "def random_geometric_augment(image: tf.Tensor) -> tf.Tensor:\n",
        "\n",
        "    # Cast constants to float32 once for calculations\n",
        "    H, W, C = IMAGE_SIZE[0], IMAGE_SIZE[1], 1\n",
        "\n",
        "    # --- 1. Rotation (Using robust tf.image.rot90) ---\n",
        "    # Apply a random rotation of 0, 90, 180, or 270 degrees.\n",
        "    k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
        "    image = tf.image.rot90(image, k=k)\n",
        "\n",
        "    # --- 2. Zooming and Shifting (Manual Implementation) ---\n",
        "\n",
        "    # A. Calculate random shifts in pixels\n",
        "    max_shift_pixels = tf.cast(H, tf.float32) * tf.cast(MAX_SHIFT, tf.float32)\n",
        "    # The result is float32. Now, you should cast the result back to int32\n",
        "    # if you want the maximum shift in whole pixels for subsequent calculations.\n",
        "    max_shift_pixels = tf.cast(max_shift_pixels, tf.int32)\n",
        "    shift_h = tf.random.uniform(shape=[], minval=-max_shift_pixels, maxval=max_shift_pixels, dtype=tf.int32)\n",
        "    shift_w = tf.random.uniform(shape=[], minval=-max_shift_pixels, maxval=max_shift_pixels, dtype=tf.int32)\n",
        "\n",
        "    # B. Apply Translation (Shift) using Padding and Cropping\n",
        "\n",
        "    # Calculate padding needed to cover the max shift\n",
        "    pad_h = tf.abs(shift_h)\n",
        "    pad_w = tf.abs(shift_w)\n",
        "\n",
        "    # Pad the image symmetrically by the maximum possible shift\n",
        "    padded_image = tf.pad(\n",
        "        image,\n",
        "        [[pad_h, pad_h], [pad_w, pad_w], [0, 0]],\n",
        "        mode='CONSTANT',\n",
        "        constant_values=0.0 # Black background for binary images\n",
        "    )\n",
        "\n",
        "    # Calculate the starting point for cropping back to the original size\n",
        "    # Start H = max_shift_pixels + shift_h (to correctly apply positive/negative shift)\n",
        "    # The starting point is the padding size plus the calculated shift.\n",
        "    start_h = pad_h + shift_h\n",
        "    start_w = pad_w + shift_w\n",
        "\n",
        "    # Crop the padded image back to the original size (128, 128)\n",
        "    image = tf.image.crop_to_bounding_box(\n",
        "        padded_image,\n",
        "        offset_height=start_h,\n",
        "        offset_width=start_w,\n",
        "        target_height=H,\n",
        "        target_width=W\n",
        "    )\n",
        "\n",
        "    # C. Apply Random Zoom\n",
        "    zoom_factor = tf.random.uniform(shape=[], minval=0.9, maxval=1.1, dtype=tf.float32)\n",
        "\n",
        "    if zoom_factor < 1.0:\n",
        "        # Zoom out (pad with black background)\n",
        "        pad_h = tf.cast(tf.cast(H, tf.float32) * (1.0 - zoom_factor) / 2.0, tf.int32)\n",
        "        pad_w = tf.cast(tf.cast(W, tf.float32) * (1.0 - zoom_factor) / 2.0, tf.int32)\n",
        "        image = tf.pad(image, [[pad_h, pad_h], [pad_w, pad_w], [0, 0]], constant_values=0.0)\n",
        "        image = tf.image.resize(image, IMAGE_SIZE, method='nearest')\n",
        "    elif zoom_factor > 1.0:\n",
        "        # Zoom in (crop and resize)\n",
        "        crop_h = tf.cast(tf.cast(H, tf.float32) / zoom_factor, tf.int32)\n",
        "        crop_w = tf.cast(tf.cast(W, tf.float32) / zoom_factor, tf.int32)\n",
        "\n",
        "        offset_h = (H - crop_h) // 2\n",
        "        offset_w = (W - crop_w) // 2\n",
        "\n",
        "        image = tf.image.crop_to_bounding_box(image, offset_h, offset_w, crop_h, crop_w)\n",
        "        image = tf.image.resize(image, IMAGE_SIZE, method='nearest')\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8x08z56TqBZu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from typing import Tuple\n",
        "\n",
        "# Define the image size based on your model architecture\n",
        "IMAGE_SIZE = (128, 128)\n",
        "AUGMENTATION_PROB = 0.5 # Probability for applying each augmentation step\n",
        "\n",
        "def load_and_augment_hybrid(\n",
        "    image_path: str, feature_vector: tf.Tensor, label: tf.Tensor, augment: bool\n",
        ") -> Tuple[Tuple[tf.Tensor, tf.Tensor], tf.Tensor]:\n",
        "    \"\"\"\n",
        "    Loads an image from path, preprocesses it, applies conditional augmentation,\n",
        "    and returns the structured inputs and label for the hybrid model.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Image Loading and Initial Processing\n",
        "    image = tf.io.read_file(filename=image_path)\n",
        "    # Decode PNG (assuming your particle images are PNG) as grayscale (1 channel)\n",
        "    image = tf.image.decode_png(contents=image, channels=1)\n",
        "\n",
        "    # Convert to float32 and resize (Normalization will happen after augmentation)\n",
        "    image = tf.image.convert_image_dtype(image=image, dtype=tf.float32)\n",
        "    image = tf.image.resize(images=image, size=IMAGE_SIZE)\n",
        "\n",
        "    # 2. Apply Augmentation (Only if 'augment' is True and based on probability)\n",
        "    if augment:\n",
        "        image = random_geometric_augment(image)\n",
        "\n",
        "    # 3. Final Normalization/Clipping\n",
        "    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n",
        "\n",
        "    # 4. Return in the format Keras model.fit expects: ( [image_input, feature_input], label )\n",
        "    return (image, feature_vector), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYTXaiFRqU4M",
        "outputId": "cedc409d-4435-4908-930e-50ee4941773c"
      },
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "print(\"Building tf.data.Dataset pipelines...\")\n",
        "\n",
        "# --- 1. Training Dataset (with augmentation) ---\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (X_paths_train, X_features_train, y_train)\n",
        ")\n",
        "\n",
        "# Apply the loading and augmentation function\n",
        "train_ds = train_ds.map(\n",
        "    lambda p, f, l: load_and_augment_hybrid(p, f, l, augment=True),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE # Parallelize data loading for speed\n",
        ")\n",
        "\n",
        "# Cache data for speed, shuffle, batch, and prefetch\n",
        "train_ds = train_ds.cache()\n",
        "train_ds = train_ds.shuffle(buffer_size=len(X_paths_train))\n",
        "train_ds = train_ds.batch(BATCH_SIZE)\n",
        "train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE) # Pre-load next batch\n",
        "\n",
        "print(\"Training Dataset prepared.\")\n",
        "\n",
        "# --- 2. Validation Dataset (no augmentation) ---\n",
        "val_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (X_paths_val, X_features_val, y_val)\n",
        ")\n",
        "\n",
        "# Apply the loading and processing function (augment=False)\n",
        "val_ds = val_ds.map(\n",
        "    lambda p, f, l: load_and_augment_hybrid(p, f, l, augment=False),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "\n",
        "# Cache data, batch, and prefetch (no shuffle needed for validation)\n",
        "val_ds = val_ds.cache()\n",
        "val_ds = val_ds.batch(BATCH_SIZE)\n",
        "val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Validation Dataset prepared.\")\n",
        "\n",
        "# --- 3. Test Dataset (for final evaluation) ---\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (X_paths_test, X_features_test, y_test)\n",
        ")\n",
        "\n",
        "# Apply the loading and processing function (augment=False)\n",
        "test_ds = test_ds.map(\n",
        "    lambda p, f, l: load_and_augment_hybrid(p, f, l, augment=False),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "\n",
        "# Batch and prefetch\n",
        "test_ds = test_ds.batch(BATCH_SIZE)\n",
        "test_ds = test_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"All datasets successfully created. Ready for model compilation and training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhRitF_GGumh"
      },
      "source": [
        "## 5. Visualize Sample Particles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bwNfOb8mGumi",
        "outputId": "14abe60c-56b5-4740-a1b7-ff3e047b29a6"
      },
      "outputs": [],
      "source": [
        "def get_class_name(label):\n",
        "    \"\"\"Convert numeric label to class name\"\"\"\n",
        "    class_names = {0: 'Liquid', 1: 'Solid'}\n",
        "    return class_names.get(label, 'Unknown')\n",
        "\n",
        "# Display random samples\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(12):\n",
        "    plt.subplot(3, 4, i+1)\n",
        "    idx = np.random.randint(0, len(X_images))\n",
        "    plt.imshow(X_images[idx, :, :, 0], cmap='gray')\n",
        "    plt.title(f'{get_class_name(y[idx])}\\nd={df_valid.iloc[idx][\"diam\"]:.1f} microns')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdwIGRSVGumi"
      },
      "source": [
        "\\## 7. Build the Hybrid CNN Model\n",
        "\n",
        "This model uses a multi-input architecture:\n",
        "- **CNN branch**: Processes particle images to extract morphological features\n",
        "- **Dense branch**: Processes environmental features (temperature, air speed, altitude)\n",
        "- **Concatenation**: Combines both branches before final classification\n",
        "\n",
        "This hybrid approach leverages both visual particle characteristics and atmospheric conditions for improved classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CSZgINBKT73u",
        "outputId": "d29f784b-44fc-4ded-867d-bfc2dd0a705c"
      },
      "outputs": [],
      "source": [
        "##Alternative simple model\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, Concatenate,SpatialDropout2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# L2 regularization strength (a common starting value)\n",
        "L2_REG = 0.0001\n",
        "\n",
        "# Simplified version - just concatenate raw features\n",
        "image_input = Input(shape=(128, 128, 1), name='image_input')\n",
        "# Reduce initial filters for simpler images\n",
        "x = Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "           kernel_regularizer=regularizers.l2(L2_REG))(image_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = SpatialDropout2D(0.2)(x)  # Spatial dropout for conv layers\n",
        "\n",
        "x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "           kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = SpatialDropout2D(0.2)(x)\n",
        "\n",
        "x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "           kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = SpatialDropout2D(0.3)(x)\n",
        "\n",
        "# Flatten the CNN output\n",
        "x = Flatten()(x)\n",
        "# Increase Dropout rate\n",
        "x = Dropout(0.4)(x)\n",
        "# Simplify and add L2 regularization\n",
        "cnn_output = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "\n",
        "feature_input = Input(shape=(len(feature_columns),), name='feature_input')\n",
        "\n",
        "# Normalize features first (important!)\n",
        "features_normalized = BatchNormalization()(feature_input)\n",
        "\n",
        "# Direct concatenation\n",
        "combined = Concatenate()([cnn_output, features_normalized])  # 130 features\n",
        "\n",
        "# Classifier\n",
        "# Simpler final layers\n",
        "z = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(combined)\n",
        "z = Dropout(0.3)(z)\n",
        "z = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(z)\n",
        "z = Dropout(0.5)(z)\n",
        "output = Dense(2, activation='softmax')(z)\n",
        "\n",
        "model = Model(inputs=[image_input, feature_input], outputs=output)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "He1B1unuvALu",
        "outputId": "dc0b087b-5dbc-4fec-e3e4-19ab6b025536"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, Concatenate,SpatialDropout2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# L2 regularization strength\n",
        "L2_REG = 0.0001\n",
        "\n",
        "image_input = Input(shape=(128, 128, 1), name='image_input')\n",
        "\n",
        "# Reduce initial filters for simpler images\n",
        "x = Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "           kernel_regularizer=regularizers.l2(L2_REG))(image_input)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = SpatialDropout2D(0.2)(x)  # Spatial dropout for conv layers\n",
        "\n",
        "x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "           kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = SpatialDropout2D(0.2)(x)\n",
        "\n",
        "x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "           kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = SpatialDropout2D(0.3)(x)\n",
        "\n",
        "# Flatten the CNN output\n",
        "x = Flatten()(x)\n",
        "# Increase Dropout rate\n",
        "x = Dropout(0.5)(x)\n",
        "# Simplify and add L2 regularization\n",
        "cnn_output = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "\n",
        "\n",
        "# Environmental features input branch\n",
        "feature_input = Input(shape=(len(feature_columns),), name='feature_input')\n",
        "\n",
        "# Normalize features first (important!)\n",
        "features_normalized = BatchNormalization()(feature_input)\n",
        "\n",
        "# Direct concatenation\n",
        "combined = Concatenate()([cnn_output, features_normalized])  # 130 features\n",
        "\n",
        "\n",
        "# Final classification layers\n",
        "# Simpler final layers\n",
        "z = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(combined)\n",
        "z = Dropout(0.5)(z)\n",
        "z = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(z)\n",
        "z = Dropout(0.3)(z)\n",
        "\n",
        "# Output layer (2-class classification: liquid, solid)\n",
        "# Assuming you switched to binary crossentropy loss:\n",
        "#output = Dense(1, activation='sigmoid', name='output')(z)\n",
        "# OR, if using categorical crossentropy for 2 classes (one-hot):\n",
        "output = Dense(2, activation='softmax', name='output')(z)\n",
        "\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=[image_input, feature_input], outputs=output, name='Hybrid_CNN_Regularized_V2')\n",
        "# Display model architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wy75eccZAVJ"
      },
      "source": [
        "k-fold cross val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        },
        "id": "oBrxJqyJZAFd",
        "outputId": "10cbda31-e47d-41d7-c9ab-db986817e982"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.model_selection import KFold\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from itertools import product\n",
        "import json\n",
        "\n",
        "# ==========================================\n",
        "# HYPERPARAMETER SEARCH SPACE\n",
        "# ==========================================\n",
        "\n",
        "param_grid = {\n",
        "    'batch_size': [8, 16, 32],\n",
        "    'learning_rate': [0.00001, 0.00005, 0.0001],\n",
        "    'l2_reg': [0.00001, 0.0001, 0.001],\n",
        "    'dropout_cnn': [0.3, 0.4, 0.5],\n",
        "    'dropout_features': [0.2, 0.3, 0.4],\n",
        "    'dropout_final': [0.4, 0.5, 0.6],\n",
        "    'cnn_dense_units': [64, 128, 256],\n",
        "    'feature_dense_units': [8, 16, 32],\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# MODEL BUILDER FUNCTION\n",
        "# ==========================================\n",
        "\n",
        "def build_hybrid_model(num_features, params):\n",
        "    \"\"\"\n",
        "    Build hybrid model with specified hyperparameters.\n",
        "\n",
        "    Parameters:\n",
        "    - num_features: Number of input features\n",
        "    - params: Dictionary of hyperparameters\n",
        "    \"\"\"\n",
        "    L2_REG = params['l2_reg']\n",
        "\n",
        "    # Image branch\n",
        "    image_input = Input(shape=(128, 128, 1), name='image_input')\n",
        "\n",
        "    x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "               kernel_regularizer=regularizers.l2(L2_REG))(image_input)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
        "\n",
        "    x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "               kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
        "\n",
        "    x = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "               kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
        "\n",
        "    x = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "               kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dropout(params['dropout_cnn'])(x)\n",
        "    cnn_output = Dense(params['cnn_dense_units'], activation='relu',\n",
        "                      kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "\n",
        "    # Feature branch\n",
        "    feature_input = Input(shape=(num_features,), name='feature_input')\n",
        "    # Normalize features first (important!)\n",
        "    features_normalized = BatchNormalization()(feature_input)\n",
        "\n",
        "    # Direct concatenation\n",
        "    combined = Concatenate()([cnn_output, features_normalized])  # 130 features\n",
        "    # Final layers\n",
        "    z = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(combined)\n",
        "    z = Dropout(params['dropout_final'])(z)\n",
        "    z = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(z)\n",
        "    z = Dropout(params['dropout_final'] * 0.6)(z)  # Slightly less dropout in second layer\n",
        "\n",
        "    output = Dense(2, activation='softmax', name='output')(z)\n",
        "\n",
        "    model = Model(inputs=[image_input, feature_input], outputs=output, name='Hybrid_CNN')\n",
        "\n",
        "    # Compile\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=params['learning_rate']),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# K-FOLD CROSS-VALIDATION FUNCTION\n",
        "# ==========================================\n",
        "\n",
        "def kfold_cross_validation(X_paths, X_features, y, params, n_splits=5,\n",
        "                           feature_columns=None, max_epochs=30, verbose=1):\n",
        "    \"\"\"\n",
        "    Perform k-fold cross-validation with given hyperparameters.\n",
        "\n",
        "    Parameters:\n",
        "    - X_paths: Array of image paths\n",
        "    - X_features: Array of feature values\n",
        "    - y: Array of labels (one-hot encoded)\n",
        "    - params: Dictionary of hyperparameters to test\n",
        "    - n_splits: Number of folds\n",
        "    - feature_columns: List of feature column names\n",
        "    - max_epochs: Maximum training epochs per fold\n",
        "    - verbose: Verbosity level\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary with CV results\n",
        "    \"\"\"\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    fold_results = []\n",
        "    fold_histories = []\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"TESTING HYPERPARAMETERS:\")\n",
        "    print(f\"{'='*80}\")\n",
        "    for key, value in params.items():\n",
        "        print(f\"  {key:20s}: {value}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_paths)):\n",
        "        print(f\"\\n--- Fold {fold_idx + 1}/{n_splits} ---\")\n",
        "\n",
        "        # Split data\n",
        "        X_paths_train, X_paths_val = X_paths[train_idx], X_paths[val_idx]\n",
        "        X_features_train, X_features_val = X_features[train_idx], X_features[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Create datasets\n",
        "        train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "            (X_paths_train, X_features_train, y_train)\n",
        "        )\n",
        "        train_ds = train_ds.map(\n",
        "            lambda p, f, l: load_and_augment_hybrid(p, f, l, augment=True),\n",
        "            num_parallel_calls=tf.data.AUTOTUNE\n",
        "        ).cache().shuffle(buffer_size=len(X_paths_train)).batch(params['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        val_ds = tf.data.Dataset.from_tensor_slices(\n",
        "            (X_paths_val, X_features_val, y_val)\n",
        "        )\n",
        "        val_ds = val_ds.map(\n",
        "            lambda p, f, l: load_and_augment_hybrid(p, f, l, augment=False),\n",
        "            num_parallel_calls=tf.data.AUTOTUNE\n",
        "        ).cache().batch(params['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        # Build model\n",
        "        model = build_hybrid_model(num_features=X_features.shape[1], params=params)\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=10,\n",
        "                restore_best_weights=True,\n",
        "                verbose=0\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=3,\n",
        "                min_lr=1e-7,\n",
        "                verbose=0\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train\n",
        "        history = model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=max_epochs,\n",
        "            callbacks=callbacks,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        train_loss, train_acc = model.evaluate(train_ds, verbose=0)\n",
        "        val_loss, val_acc = model.evaluate(val_ds, verbose=0)\n",
        "\n",
        "        # Store results\n",
        "        fold_results.append({\n",
        "            'fold': fold_idx + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'train_acc': train_acc,\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc': val_acc,\n",
        "            'best_epoch': np.argmin(history.history['val_loss']) + 1,\n",
        "            'final_epoch': len(history.history['loss'])\n",
        "        })\n",
        "\n",
        "        fold_histories.append(history.history)\n",
        "\n",
        "        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.4f}\")\n",
        "        print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.4f}\")\n",
        "\n",
        "    # Aggregate results\n",
        "    results_df = pd.DataFrame(fold_results)\n",
        "\n",
        "    cv_results = {\n",
        "        'params': params,\n",
        "        'mean_val_acc': results_df['val_acc'].mean(),\n",
        "        'std_val_acc': results_df['val_acc'].std(),\n",
        "        'mean_train_acc': results_df['train_acc'].mean(),\n",
        "        'std_train_acc': results_df['train_acc'].std(),\n",
        "        'mean_val_loss': results_df['val_loss'].mean(),\n",
        "        'std_val_loss': results_df['val_loss'].std(),\n",
        "        'overfitting_gap': results_df['train_acc'].mean() - results_df['val_acc'].mean(),\n",
        "        'fold_results': fold_results,\n",
        "        'fold_histories': fold_histories\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"CROSS-VALIDATION SUMMARY:\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Mean Val Accuracy:  {cv_results['mean_val_acc']:.4f} ± {cv_results['std_val_acc']:.4f}\")\n",
        "    print(f\"Mean Train Accuracy: {cv_results['mean_train_acc']:.4f} ± {cv_results['std_train_acc']:.4f}\")\n",
        "    print(f\"Overfitting Gap:     {cv_results['overfitting_gap']:.4f}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    return cv_results\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# GRID SEARCH WITH K-FOLD CV\n",
        "# ==========================================\n",
        "\n",
        "def grid_search_kfold(X_paths, X_features, y, param_grid, n_splits=5,\n",
        "                     feature_columns=None, max_epochs=30, max_combinations=None):\n",
        "    \"\"\"\n",
        "    Perform grid search with k-fold cross-validation.\n",
        "\n",
        "    Parameters:\n",
        "    - X_paths: Array of image paths\n",
        "    - X_features: Array of feature values\n",
        "    - y: Array of labels\n",
        "    - param_grid: Dictionary of hyperparameter lists to search\n",
        "    - n_splits: Number of CV folds\n",
        "    - feature_columns: List of feature column names\n",
        "    - max_epochs: Maximum epochs per fold\n",
        "    - max_combinations: Limit number of combinations to test (None = all)\n",
        "\n",
        "    Returns:\n",
        "    - results_df: DataFrame with all results\n",
        "    - best_params: Best hyperparameter combination\n",
        "    \"\"\"\n",
        "    # Generate all combinations\n",
        "    param_names = list(param_grid.keys())\n",
        "    param_values = list(param_grid.values())\n",
        "    all_combinations = list(product(*param_values))\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"GRID SEARCH WITH {n_splits}-FOLD CROSS-VALIDATION\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Total combinations to test: {len(all_combinations)}\")\n",
        "\n",
        "    if max_combinations and len(all_combinations) > max_combinations:\n",
        "        print(f\"Limiting to {max_combinations} random combinations\")\n",
        "        np.random.shuffle(all_combinations)\n",
        "        all_combinations = all_combinations[:max_combinations]\n",
        "\n",
        "    print(f\"Testing {len(all_combinations)} combinations...\")\n",
        "    print(f\"Estimated time: ~{len(all_combinations) * n_splits * 5} minutes\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for idx, param_combo in enumerate(all_combinations):\n",
        "        print(f\"\\n{'#'*80}\")\n",
        "        print(f\"COMBINATION {idx + 1}/{len(all_combinations)}\")\n",
        "        print(f\"{'#'*80}\")\n",
        "\n",
        "        # Create params dict\n",
        "        params = {name: value for name, value in zip(param_names, param_combo)}\n",
        "\n",
        "        # Run k-fold CV\n",
        "        cv_results = kfold_cross_validation(\n",
        "            X_paths, X_features, y, params,\n",
        "            n_splits=n_splits,\n",
        "            feature_columns=feature_columns,\n",
        "            max_epochs=max_epochs,\n",
        "            verbose=0  # Less verbose for grid search\n",
        "        )\n",
        "\n",
        "        all_results.append(cv_results)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    results_data = []\n",
        "    for result in all_results:\n",
        "        row = result['params'].copy()\n",
        "        row.update({\n",
        "            'mean_val_acc': result['mean_val_acc'],\n",
        "            'std_val_acc': result['std_val_acc'],\n",
        "            'mean_train_acc': result['mean_train_acc'],\n",
        "            'overfitting_gap': result['overfitting_gap'],\n",
        "            'mean_val_loss': result['mean_val_loss']\n",
        "        })\n",
        "        results_data.append(row)\n",
        "\n",
        "    results_df = pd.DataFrame(results_data)\n",
        "    results_df = results_df.sort_values('mean_val_acc', ascending=False)\n",
        "\n",
        "    # Get best params\n",
        "    best_params = results_df.iloc[0][list(param_grid.keys())].to_dict()\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"GRID SEARCH COMPLETE\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\nBest Validation Accuracy: {results_df.iloc[0]['mean_val_acc']:.4f} \"\n",
        "          f\"± {results_df.iloc[0]['std_val_acc']:.4f}\")\n",
        "    print(f\"\\nBest Hyperparameters:\")\n",
        "    for key, value in best_params.items():\n",
        "        print(f\"  {key:20s}: {value}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    return results_df, best_params, all_results\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# VISUALIZATION FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def plot_grid_search_results(results_df, save_path='grid_search_results.png'):\n",
        "    \"\"\"Visualize grid search results.\"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "    # Plot 1: Top 10 configurations\n",
        "    ax1 = axes[0, 0]\n",
        "    top_10 = results_df.head(10)\n",
        "    y_pos = np.arange(len(top_10))\n",
        "    ax1.barh(y_pos, top_10['mean_val_acc'], xerr=top_10['std_val_acc'],\n",
        "             color='steelblue', alpha=0.7, capsize=5)\n",
        "    ax1.set_yticks(y_pos)\n",
        "    ax1.set_yticklabels([f\"Config {i+1}\" for i in range(len(top_10))])\n",
        "    ax1.set_xlabel('Validation Accuracy', fontweight='bold')\n",
        "    ax1.set_title('Top 10 Configurations', fontweight='bold')\n",
        "    ax1.invert_yaxis()\n",
        "    ax1.grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # Plot 2: Learning rate effect\n",
        "    ax2 = axes[0, 1]\n",
        "    lr_groups = results_df.groupby('learning_rate')['mean_val_acc'].agg(['mean', 'std'])\n",
        "    ax2.errorbar(lr_groups.index, lr_groups['mean'], yerr=lr_groups['std'],\n",
        "                 marker='o', capsize=5, linewidth=2, markersize=8)\n",
        "    ax2.set_xlabel('Learning Rate', fontweight='bold')\n",
        "    ax2.set_ylabel('Mean Validation Accuracy', fontweight='bold')\n",
        "    ax2.set_title('Learning Rate Effect', fontweight='bold')\n",
        "    ax2.set_xscale('log')\n",
        "    ax2.grid(alpha=0.3)\n",
        "\n",
        "    # Plot 3: Batch size effect\n",
        "    ax3 = axes[0, 2]\n",
        "    bs_groups = results_df.groupby('batch_size')['mean_val_acc'].agg(['mean', 'std'])\n",
        "    ax3.errorbar(bs_groups.index, bs_groups['mean'], yerr=bs_groups['std'],\n",
        "                 marker='s', capsize=5, linewidth=2, markersize=8, color='coral')\n",
        "    ax3.set_xlabel('Batch Size', fontweight='bold')\n",
        "    ax3.set_ylabel('Mean Validation Accuracy', fontweight='bold')\n",
        "    ax3.set_title('Batch Size Effect', fontweight='bold')\n",
        "    ax3.grid(alpha=0.3)\n",
        "\n",
        "    # Plot 4: L2 regularization effect\n",
        "    ax4 = axes[1, 0]\n",
        "    l2_groups = results_df.groupby('l2_reg')['mean_val_acc'].agg(['mean', 'std'])\n",
        "    ax4.errorbar(l2_groups.index, l2_groups['mean'], yerr=l2_groups['std'],\n",
        "                 marker='^', capsize=5, linewidth=2, markersize=8, color='green')\n",
        "    ax4.set_xlabel('L2 Regularization', fontweight='bold')\n",
        "    ax4.set_ylabel('Mean Validation Accuracy', fontweight='bold')\n",
        "    ax4.set_title('L2 Regularization Effect', fontweight='bold')\n",
        "    ax4.set_xscale('log')\n",
        "    ax4.grid(alpha=0.3)\n",
        "\n",
        "    # Plot 5: Overfitting gap\n",
        "    ax5 = axes[1, 1]\n",
        "    ax5.scatter(results_df['mean_val_acc'], results_df['overfitting_gap'],\n",
        "               alpha=0.6, s=100, c=results_df['batch_size'], cmap='viridis')\n",
        "    ax5.axhline(y=0, color='red', linestyle='--', linewidth=2, label='No overfitting')\n",
        "    ax5.set_xlabel('Validation Accuracy', fontweight='bold')\n",
        "    ax5.set_ylabel('Overfitting Gap (Train - Val)', fontweight='bold')\n",
        "    ax5.set_title('Overfitting Analysis', fontweight='bold')\n",
        "    ax5.legend()\n",
        "    ax5.grid(alpha=0.3)\n",
        "    cbar = plt.colorbar(ax5.collections[0], ax=ax5)\n",
        "    cbar.set_label('Batch Size', fontweight='bold')\n",
        "\n",
        "    # Plot 6: Accuracy vs Loss\n",
        "    ax6 = axes[1, 2]\n",
        "    scatter = ax6.scatter(results_df['mean_val_loss'], results_df['mean_val_acc'],\n",
        "                         alpha=0.6, s=100, c=results_df['learning_rate'],\n",
        "                         cmap='plasma', norm=plt.matplotlib.colors.LogNorm())\n",
        "    ax6.set_xlabel('Validation Loss', fontweight='bold')\n",
        "    ax6.set_ylabel('Validation Accuracy', fontweight='bold')\n",
        "    ax6.set_title('Loss vs Accuracy', fontweight='bold')\n",
        "    ax6.grid(alpha=0.3)\n",
        "    cbar = plt.colorbar(scatter, ax=ax6)\n",
        "    cbar.set_label('Learning Rate', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"✓ Grid search visualization saved to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_cv_fold_variance(all_results, best_idx=0, save_path='cv_fold_variance.png'):\n",
        "    \"\"\"Plot variance across CV folds for best configuration.\"\"\"\n",
        "    best_result = all_results[best_idx]\n",
        "    fold_results = pd.DataFrame(best_result['fold_results'])\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Plot 1: Accuracy across folds\n",
        "    ax1 = axes[0]\n",
        "    x = fold_results['fold']\n",
        "    ax1.plot(x, fold_results['train_acc'], 'o-', label='Train', linewidth=2, markersize=8)\n",
        "    ax1.plot(x, fold_results['val_acc'], 's-', label='Validation', linewidth=2, markersize=8)\n",
        "    ax1.fill_between(x,\n",
        "                     fold_results['val_acc'].mean() - fold_results['val_acc'].std(),\n",
        "                     fold_results['val_acc'].mean() + fold_results['val_acc'].std(),\n",
        "                     alpha=0.2)\n",
        "    ax1.set_xlabel('Fold', fontweight='bold')\n",
        "    ax1.set_ylabel('Accuracy', fontweight='bold')\n",
        "    ax1.set_title(f\"Accuracy Across CV Folds\\nMean Val: {fold_results['val_acc'].mean():.4f} ± {fold_results['val_acc'].std():.4f}\",\n",
        "                 fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(alpha=0.3)\n",
        "    ax1.set_xticks(x)\n",
        "\n",
        "    # Plot 2: Loss across folds\n",
        "    ax2 = axes[1]\n",
        "    ax2.plot(x, fold_results['train_loss'], 'o-', label='Train', linewidth=2, markersize=8)\n",
        "    ax2.plot(x, fold_results['val_loss'], 's-', label='Validation', linewidth=2, markersize=8)\n",
        "    ax2.fill_between(x,\n",
        "                     fold_results['val_loss'].mean() - fold_results['val_loss'].std(),\n",
        "                     fold_results['val_loss'].mean() + fold_results['val_loss'].std(),\n",
        "                     alpha=0.2)\n",
        "    ax2.set_xlabel('Fold', fontweight='bold')\n",
        "    ax2.set_ylabel('Loss', fontweight='bold')\n",
        "    ax2.set_title(f\"Loss Across CV Folds\\nMean Val: {fold_results['val_loss'].mean():.4f} ± {fold_results['val_loss'].std():.4f}\",\n",
        "                 fontweight='bold')\n",
        "    ax2.legend()\n",
        "    ax2.grid(alpha=0.3)\n",
        "    ax2.set_xticks(x)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"✓ CV fold variance plot saved to {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# MAIN EXECUTION\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"HYBRID CNN HYPERPARAMETER OPTIMIZATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Combine train + val for cross-validation\n",
        "    # (Keep test set completely separate!)\n",
        "    X_paths_cv = np.concatenate([X_paths_train, X_paths_val])\n",
        "    X_features_cv = np.concatenate([X_features_train, X_features_val])\n",
        "    y_cv = np.concatenate([y_train, y_val])\n",
        "\n",
        "    print(f\"\\nCombined dataset for CV: {len(X_paths_cv)} samples\")\n",
        "    print(f\"Test set (held out): {len(X_paths_test)} samples\")\n",
        "\n",
        "    # Option 1: Quick search (smaller grid)\n",
        "    print(\"\\n--- OPTION 1: QUICK SEARCH ---\")\n",
        "    quick_param_grid = {\n",
        "        'batch_size': [16, 32],\n",
        "        'learning_rate': [0.00005, 0.0001],\n",
        "        'l2_reg': [0.0001],\n",
        "        'dropout_cnn': [0.4],\n",
        "        'dropout_features': [0.3],\n",
        "        'dropout_final': [0.5],\n",
        "        'cnn_dense_units': [128],\n",
        "        'feature_dense_units': [16],\n",
        "    }\n",
        "\n",
        "    # Run quick search (2 x 2 = 4 combinations)\n",
        "    results_df_quick, best_params_quick, all_results_quick = grid_search_kfold(\n",
        "        X_paths_cv, X_features_cv, y_cv,\n",
        "        param_grid=quick_param_grid,\n",
        "        n_splits=5,\n",
        "        feature_columns=feature_columns,\n",
        "        max_epochs=30\n",
        "    )\n",
        "\n",
        "    # Visualize results\n",
        "    plot_grid_search_results(results_df_quick, save_path='quick_search_results.png')\n",
        "    plot_cv_fold_variance(all_results_quick, best_idx=0, save_path='quick_search_cv_folds.png')\n",
        "\n",
        "    # Save results\n",
        "    results_df_quick.to_csv('quick_search_results.csv', index=False)\n",
        "\n",
        "    with open('best_params_quick.json', 'w') as f:\n",
        "        json.dump(best_params_quick, f, indent=2)\n",
        "\n",
        "    print(\"\\n✓ Quick search complete!\")\n",
        "    print(\"✓ Results saved to 'quick_search_results.csv'\")\n",
        "    print(\"✓ Best parameters saved to 'best_params_quick.json'\")\n",
        "\n",
        "    # Option 2: Full search (if you have time - this will take hours!)\n",
        "    run_full_search = input(\"\\nRun full grid search? This will take several hours. (y/n): \")\n",
        "\n",
        "    if run_full_search.lower() == 'y':\n",
        "        print(\"\\n--- OPTION 2: FULL SEARCH ---\")\n",
        "\n",
        "        results_df_full, best_params_full, all_results_full = grid_search_kfold(\n",
        "            X_paths_cv, X_features_cv, y_cv,\n",
        "            param_grid=param_grid,  # Full grid\n",
        "            n_splits=5,\n",
        "            feature_columns=feature_columns,\n",
        "            max_epochs=30,\n",
        "            max_combinations=50  # Limit to 50 random combinations\n",
        "        )\n",
        "\n",
        "        # Visualize\n",
        "        plot_grid_search_results(results_df_full, save_path='full_search_results.png')\n",
        "        plot_cv_fold_variance(all_results_full, best_idx=0, save_path='full_search_cv_folds.png')\n",
        "\n",
        "        # Save\n",
        "        results_df_full.to_csv('full_search_results.csv', index=False)\n",
        "\n",
        "        with open('best_params_full.json', 'w') as f:\n",
        "            json.dump(best_params_full, f, indent=2)\n",
        "\n",
        "        print(\"\\n✓ Full search complete!\")\n",
        "        print(\"✓ Results saved to 'full_search_results.csv'\")\n",
        "        print(\"✓ Best parameters saved to 'best_params_full.json'\")\n",
        "\n",
        "    # ==========================================\n",
        "    # TRAIN FINAL MODEL WITH BEST PARAMS\n",
        "    # ==========================================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRAINING FINAL MODEL WITH BEST PARAMETERS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Use best params from quick search\n",
        "    best_params = best_params_quick\n",
        "\n",
        "    # Recreate train/val split\n",
        "    train_ds_final = tf.data.Dataset.from_tensor_slices(\n",
        "        (X_paths_train, X_features_train, y_train)\n",
        "    ).map(\n",
        "        lambda p, f, l: load_and_augment_hybrid(p, f, l, augment=True),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).cache().shuffle(buffer_size=len(X_paths_train)).batch(best_params['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    val_ds_final = tf.data.Dataset.from_tensor_slices(\n",
        "        (X_paths_val, X_features_val, y_val)\n",
        "    ).map(\n",
        "        lambda p, f, l: load_and_augment_hybrid(p, f, l, augment=False),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).cache().batch(best_params['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    test_ds_final = tf.data.Dataset.from_tensor_slices(\n",
        "        (X_paths_test, X_features_test, y_test)\n",
        "    ).map(\n",
        "        lambda p, f, l: load_and_augment_hybrid(p, f, l, augment=False),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).batch(best_params['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # Build and train final model\n",
        "    final_model = build_hybrid_model(num_features=X_features_train.shape[1], params=best_params)\n",
        "\n",
        "    callbacks_final = [\n",
        "        EarlyStopping(patience=15, restore_best_weights=True, verbose=1),\n",
        "        ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-7, verbose=1)\n",
        "    ]\n",
        "\n",
        "    history_final = final_model.fit(\n",
        "        train_ds_final,\n",
        "        validation_data=val_ds_final,\n",
        "        epochs=50,\n",
        "        callbacks=callbacks_final,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss, test_acc = final_model.evaluate(test_ds_final)\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"FINAL MODEL PERFORMANCE\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    # Save final model\n",
        "    final_model.save('final_optimized_model.h5')\n",
        "    print(\"✓ Final model saved to 'final_optimized_model.h5'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CXejxeRGumi"
      },
      "source": [
        "## 8. Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0JrptfGNwAe"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau# Exponential learning rate decay\n",
        "\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,              # Reduce LR by half\n",
        "    patience=3,              # Wait 3 epochs before reducing\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbTxiswS3oeH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
        "def compile_model(model):\n",
        "    \"\"\"\n",
        "    Compile the model with appropriate loss function and optimizer.\n",
        "    \"\"\"\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.0001),  # Start lower\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', Precision(), Recall(), AUC()]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuPV6bYtGumi"
      },
      "outputs": [],
      "source": [
        "# Compile with appropriate loss function and optimizer\n",
        "\n",
        "compile_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwaMZebuGumi"
      },
      "source": [
        "## 9. Set Up Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1mC1HCcGumi",
        "outputId": "687fdfa4-c0b4-4531-dbe8-82c9a32a7303"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 1. Early Stopping (Updated Patience) ---\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_accuracy', # Monitors the metric you want to minimize\n",
        "    mode='max',         # Stop when val_loss stops decreasing\n",
        "    patience=10,        # Reduced patience to 10 epochs\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# --- 2. Model Checkpoint (Updated Monitoring and Naming) ---\n",
        "checkpoint = ModelCheckpoint(\n",
        "    # Updated file name for clarity (assuming binary classification)\n",
        "    'best_particle_classifier_hybrid_binary.keras',\n",
        "    monitor='val_accuracy', # Changed to monitor val_loss, matching EarlyStopping\n",
        "    mode='max',         # Save when val_loss is at its minimum\n",
        "    verbose=1,\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "print(\"Callbacks configured for robust training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbllFZmLGumi"
      },
      "source": [
        "## 10. Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB5JopEYGumi",
        "outputId": "52f4718b-7768-46c7-fa5d-0e4d693756f6"
      },
      "outputs": [],
      "source": [
        "# --- Calculate Steps Per Epoch ---\n",
        "# When using tf.data.Dataset, you need to tell Keras how many batches\n",
        "# make up one epoch.\n",
        "N_TRAIN_SAMPLES = len(X_paths_train) # Use the count of training samples\n",
        "N_VAL_SAMPLES = len(X_paths_val)     # Use the count of validation samples\n",
        "N_EPOCHS = 100\n",
        "steps_per_epoch = N_TRAIN_SAMPLES // BATCH_SIZE\n",
        "# Use validation_steps to ensure validation is done over the entire set\n",
        "validation_steps = N_VAL_SAMPLES // BATCH_SIZE\n",
        "# If the number of samples is not perfectly divisible, you might add 1 to the steps\n",
        "if N_TRAIN_SAMPLES % BATCH_SIZE != 0:\n",
        "    steps_per_epoch += 1\n",
        "if N_VAL_SAMPLES % BATCH_SIZE != 0:\n",
        "    validation_steps += 1\n",
        "# --- Train the model with class weights using the Datasets ---\n",
        "print(\"Starting training with class weights...\")\n",
        "print(f\"Using class weights:\")\n",
        "for i, w in class_weight_dict.items():\n",
        "    print(f\"  Class {i} ({get_class_name(i)}): {w:.2f}\")\n",
        "\n",
        "history = model.fit(\n",
        "    # 1. Pass the training dataset object\n",
        "    train_ds,\n",
        "    # 2. Set the number of steps per epoch\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=N_EPOCHS,\n",
        "    # 3. Pass the validation dataset object\n",
        "    validation_data=val_ds,\n",
        "    # 4. Set the validation steps\n",
        "    validation_steps=validation_steps,\n",
        "    # 5. Keep class weights (applied per batch)\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[early_stop, checkpoint, lr_scheduler],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNFU-co8Gumi"
      },
      "source": [
        "## 11. Visualize Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "zfuBCuPOGumi",
        "outputId": "5c7a8a35-866f-46e4-cb4c-7d8b86a1307a"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot accuracy\n",
        "axes[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
        "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "axes[0].set_title('Model Accuracy (Hybrid CNN)')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Plot loss\n",
        "axes[1].plot(history.history['loss'], label='Training Loss')\n",
        "axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
        "axes[1].set_title('Model Loss (Hybrid CNN)')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final metrics\n",
        "print(f\"\\nFinal Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
        "print(f\"Best Validation Accuracy: {max(history.history['val_accuracy']):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXA9BlLzw689",
        "outputId": "f56a8d2e-5b77-4068-ab54-8fe09925d28f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming your test data is processed and synchronized as NumPy arrays for simplicity\n",
        "# If using the test_ds generator, you must iterate it to get all X and y_true values.\n",
        "# For simplicity, we assume you have the test arrays: X_features_test, X_paths_test, y_test\n",
        "# and load the full test data here (or use the test_ds object)\n",
        "\n",
        "# 1. Get the true labels\n",
        "# If y_test is one-hot (e.g., [[1, 0], [0, 1]]), extract the positive class column (index 1)\n",
        "y_true = y_test[:, 1]\n",
        "\n",
        "# 2. Get the model's probability predictions for the test set\n",
        "# If using the tf.data.Dataset pipeline:\n",
        "y_pred_probs = model.predict(test_ds)\n",
        "\n",
        "# If your model output is Dense(1, sigmoid), y_pred_probs is already (N_samples, 1)\n",
        "# If your model output is Dense(2, softmax), y_pred_probs has shape (N_samples, 2).\n",
        "# We take the probability of the positive class (index 1).\n",
        "if y_pred_probs.shape[1] > 1:\n",
        "    y_scores = y_pred_probs[:, 1]\n",
        "else:\n",
        "    y_scores = y_pred_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "XkbJVZCYxEEg",
        "outputId": "4fc242ae-5cb6-42e9-e0e9-01a8ce77442b"
      },
      "outputs": [],
      "source": [
        "# Calculate the True Positive Rate (TPR) and False Positive Rate (FPR)\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "\n",
        "# Calculate the AUC\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "         label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guess (AUC = 0.5)')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR) / Recall')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmK_N-pAGumi"
      },
      "source": [
        "## 12. Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr-OTfFIGumi",
        "outputId": "09d5924e-d045-4457-859d-ca555156a456"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "from keras.models import load_model\n",
        "\n",
        "best_model = load_model('best_particle_classifier_hybrid_binary.keras')\n",
        "\n",
        "# Evaluate on training set\n",
        "train_metrics = best_model.evaluate(train_ds, verbose=0)\n",
        "# Evaluate on validation set\n",
        "val_metrics = best_model.evaluate(val_ds, verbose=0)\n",
        "# Evaluate on test set\n",
        "test_metrics = best_model.evaluate(test_ds, verbose=0)\n",
        "\n",
        "# The metrics list order is: [Loss, Accuracy, Precision, Recall, AUC]\n",
        "# Note: Ensure your model was compiled with these metrics for this to work\n",
        "metric_names = ['Loss', 'Accuracy', 'Precision', 'Recall', 'AUC']\n",
        "\n",
        "# --- 3. Print Results ---\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"HYBRID BINARY CNN PERFORMANCE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def print_metrics(set_name, metrics):\n",
        "    \"\"\"Helper function to print formatted metrics.\"\"\"\n",
        "    print(f\"\\n--- {set_name} Set ---\")\n",
        "    for name, value in zip(metric_names, metrics):\n",
        "        if name == 'Accuracy':\n",
        "            print(f'{name}: {value:.4f} ({value*100:.2f}%)')\n",
        "        else:\n",
        "            print(f'{name}: {value:.4f}')\n",
        "\n",
        "print_metrics(\"Training\", train_metrics)\n",
        "print_metrics(\"Validation\", val_metrics)\n",
        "print_metrics(\"Test\", test_metrics)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "evA8ObwJzuMm",
        "outputId": "3fc0a386-2162-4f8b-d3c7-48e35b082623"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def get_class_name(class_idx):\n",
        "    \"\"\"Convert class index to name.\"\"\"\n",
        "    return 'Liquid' if class_idx == 0 else 'Solid'\n",
        "\n",
        "\n",
        "def get_all_images_from_dataset(test_ds):\n",
        "    \"\"\"Extract all images from batched test dataset.\"\"\"\n",
        "    all_images = []\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in test_ds:\n",
        "        (images, features), labels = batch\n",
        "        all_images.append(images.numpy())\n",
        "        all_features.append(features.numpy())\n",
        "        all_labels.append(labels.numpy())\n",
        "\n",
        "    all_images = np.concatenate(all_images, axis=0)\n",
        "    all_features = np.concatenate(all_features, axis=0)\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "    return all_images, all_features, all_labels\n",
        "\n",
        "\n",
        "# Extract images and labels from test dataset\n",
        "test_images, test_features, test_labels = get_all_images_from_dataset(test_ds)\n",
        "print(f\"Extracted {len(test_images)} test images\")\n",
        "\n",
        "# Convert one-hot labels to class indices\n",
        "y_true = np.argmax(test_labels, axis=1)\n",
        "\n",
        "\n",
        "def plot_misclassified_particles(y_true, y_pred, y_pred_probs, test_images, test_features,\n",
        "                                  feature_columns, max_display=16):\n",
        "    \"\"\"\n",
        "    Plot all misclassified particles with detailed information.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: True labels (class indices)\n",
        "    - y_pred: Predicted labels (class indices)\n",
        "    - y_pred_probs: Prediction probabilities\n",
        "    - test_images: Test images array\n",
        "    - test_features: Test features array\n",
        "    - feature_columns: List of feature names\n",
        "    - max_display: Maximum number of samples to display\n",
        "    \"\"\"\n",
        "    # Find misclassified indices\n",
        "    misclassified_idx = np.where(y_pred != y_true)[0]\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"MISCLASSIFICATION ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Total test samples: {len(y_true)}\")\n",
        "    print(f\"Misclassified: {len(misclassified_idx)} ({len(misclassified_idx)/len(y_true)*100:.2f}%)\")\n",
        "    print()\n",
        "\n",
        "    # Analyze misclassification patterns\n",
        "    # Liquid misclassified as Solid\n",
        "    liquid_as_solid = np.where((y_true == 0) & (y_pred == 1))[0]\n",
        "    # Solid misclassified as Liquid\n",
        "    solid_as_liquid = np.where((y_true == 1) & (y_pred == 0))[0]\n",
        "\n",
        "    print(f\"Liquid → Solid errors: {len(liquid_as_solid)} ({len(liquid_as_solid)/np.sum(y_true==0)*100:.1f}% of liquids)\")\n",
        "    print(f\"Solid → Liquid errors: {len(solid_as_liquid)} ({len(solid_as_liquid)/np.sum(y_true==1)*100:.1f}% of solids)\")\n",
        "    print()\n",
        "\n",
        "    if len(misclassified_idx) == 0:\n",
        "        print(\"🎉 No misclassifications! Perfect model!\")\n",
        "        return\n",
        "\n",
        "    # Limit display\n",
        "    display_idx = misclassified_idx[:max_display]\n",
        "    n_samples = len(display_idx)\n",
        "\n",
        "    # Calculate grid size\n",
        "    n_cols = 4\n",
        "    n_rows = int(np.ceil(n_samples / n_cols))\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 5*n_rows))\n",
        "\n",
        "    for plot_idx, sample_idx in enumerate(display_idx):\n",
        "        ax = plt.subplot(n_rows, n_cols, plot_idx + 1)\n",
        "\n",
        "        # Get data\n",
        "        image = test_images[sample_idx, :, :, 0]\n",
        "        true_class = y_true[sample_idx]\n",
        "        pred_class = y_pred[sample_idx]\n",
        "        confidence = y_pred_probs[sample_idx, pred_class]\n",
        "\n",
        "        # Get feature values\n",
        "        features = test_features[sample_idx]\n",
        "\n",
        "        # Display image\n",
        "        ax.imshow(image, cmap='gray')\n",
        "\n",
        "        # Create detailed title\n",
        "        title = f\"Sample #{sample_idx}\\n\"\n",
        "        title += f\"TRUE: {get_class_name(true_class)} | PRED: {get_class_name(pred_class)}\\n\"\n",
        "        title += f\"Confidence: {confidence:.3f}\\n\"\n",
        "        title += f\"Probs: [L:{y_pred_probs[sample_idx, 0]:.3f}, S:{y_pred_probs[sample_idx, 1]:.3f}]\\n\"\n",
        "\n",
        "        # Add feature information\n",
        "        if len(feature_columns) == 1:\n",
        "            title += f\"{feature_columns[0]}: {features[0]:.1f}\"\n",
        "        else:\n",
        "            feature_str = \", \".join([f\"{name}:{val:.1f}\" for name, val in zip(feature_columns, features)])\n",
        "            title += f\"{feature_str}\"\n",
        "\n",
        "        ax.set_title(title, fontsize=10, color='red', weight='bold')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.suptitle(f'Misclassified Particles (Showing {n_samples} of {len(misclassified_idx)})',\n",
        "                 fontsize=16, weight='bold', y=1.0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('misclassified_particles.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed statistics\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"DETAILED MISCLASSIFICATION STATISTICS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    if len(misclassified_idx) > 0:\n",
        "        # Confidence analysis\n",
        "        misclass_confidences = y_pred_probs[misclassified_idx, y_pred[misclassified_idx]]\n",
        "        correct_idx = np.where(y_pred == y_true)[0]\n",
        "        correct_confidences = y_pred_probs[correct_idx, y_pred[correct_idx]]\n",
        "\n",
        "        print(f\"\\nConfidence Analysis:\")\n",
        "        print(f\"  Misclassified - Mean confidence: {np.mean(misclass_confidences):.3f} (±{np.std(misclass_confidences):.3f})\")\n",
        "        print(f\"  Correct       - Mean confidence: {np.mean(correct_confidences):.3f} (±{np.std(correct_confidences):.3f})\")\n",
        "\n",
        "        # Feature analysis for misclassified samples\n",
        "        print(f\"\\nFeature Statistics for Misclassified Samples:\")\n",
        "        misclass_features = test_features[misclassified_idx]\n",
        "\n",
        "        for i, feat_name in enumerate(feature_columns):\n",
        "            if len(feature_columns) == 1:\n",
        "                feat_values = misclass_features\n",
        "            else:\n",
        "                feat_values = misclass_features[:, i]\n",
        "            print(f\"  {feat_name}: mean={np.mean(feat_values):.2f}, \"\n",
        "                  f\"std={np.std(feat_values):.2f}, \"\n",
        "                  f\"range=[{np.min(feat_values):.2f}, {np.max(feat_values):.2f}]\")\n",
        "\n",
        "\n",
        "def plot_confidence_distribution(y_true, y_pred, y_pred_probs):\n",
        "    \"\"\"\n",
        "    Plot confidence distributions for correct vs incorrect predictions.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Get confidences\n",
        "    confidences = np.max(y_pred_probs, axis=1)\n",
        "    correct_mask = (y_pred == y_true)\n",
        "\n",
        "    correct_conf = confidences[correct_mask]\n",
        "    incorrect_conf = confidences[~correct_mask]\n",
        "\n",
        "    # Plot 1: Confidence histograms\n",
        "    ax1 = axes[0]\n",
        "    ax1.hist(correct_conf, bins=20, alpha=0.7, label='Correct', color='green', edgecolor='black')\n",
        "    ax1.hist(incorrect_conf, bins=20, alpha=0.7, label='Incorrect', color='red', edgecolor='black')\n",
        "    ax1.set_xlabel('Prediction Confidence', fontweight='bold')\n",
        "    ax1.set_ylabel('Count', fontweight='bold')\n",
        "    ax1.set_title('Confidence Distribution: Correct vs Incorrect', fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(alpha=0.3)\n",
        "\n",
        "    # Plot 2: Confidence by class\n",
        "    ax2 = axes[1]\n",
        "\n",
        "    # Liquid predictions\n",
        "    liquid_correct = confidences[(y_pred == 0) & correct_mask]\n",
        "    liquid_incorrect = confidences[(y_pred == 0) & ~correct_mask]\n",
        "\n",
        "    # Solid predictions\n",
        "    solid_correct = confidences[(y_pred == 1) & correct_mask]\n",
        "    solid_incorrect = confidences[(y_pred == 1) & ~correct_mask]\n",
        "\n",
        "    positions = [1, 2, 4, 5]\n",
        "    bp = ax2.boxplot([liquid_correct, liquid_incorrect, solid_correct, solid_incorrect],\n",
        "                      positions=positions,\n",
        "                      widths=0.6,\n",
        "                      patch_artist=True,\n",
        "                      labels=['Liquid\\nCorrect', 'Liquid\\nIncorrect',\n",
        "                              'Solid\\nCorrect', 'Solid\\nIncorrect'])\n",
        "\n",
        "    # Color the boxes\n",
        "    colors = ['lightgreen', 'lightcoral', 'lightgreen', 'lightcoral']\n",
        "    for patch, color in zip(bp['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "\n",
        "    ax2.set_ylabel('Prediction Confidence', fontweight='bold')\n",
        "    ax2.set_title('Confidence by Class and Correctness', fontweight='bold')\n",
        "    ax2.grid(alpha=0.3, axis='y')\n",
        "    ax2.set_ylim([0, 1])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confidence_analysis.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_feature_distributions_by_correctness(y_true, y_pred, test_features, feature_columns):\n",
        "    \"\"\"\n",
        "    Compare feature distributions between correctly and incorrectly classified samples.\n",
        "    \"\"\"\n",
        "    correct_mask = (y_pred == y_true)\n",
        "\n",
        "    n_features = len(feature_columns) if len(test_features.shape) > 1 else 1\n",
        "\n",
        "    if n_features == 1:\n",
        "        feature_columns_list = feature_columns\n",
        "        test_features_array = test_features.reshape(-1, 1)\n",
        "    else:\n",
        "        feature_columns_list = feature_columns\n",
        "        test_features_array = test_features\n",
        "\n",
        "    fig, axes = plt.subplots(1, n_features, figsize=(6*n_features, 5))\n",
        "\n",
        "    if n_features == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, (ax, feat_name) in enumerate(zip(axes, feature_columns_list)):\n",
        "        feat_values = test_features_array[:, i]\n",
        "\n",
        "        correct_vals = feat_values[correct_mask]\n",
        "        incorrect_vals = feat_values[~correct_mask]\n",
        "\n",
        "        ax.hist(correct_vals, bins=20, alpha=0.6, label='Correct', color='green', edgecolor='black')\n",
        "        ax.hist(incorrect_vals, bins=20, alpha=0.6, label='Incorrect', color='red', edgecolor='black')\n",
        "\n",
        "        ax.set_xlabel(feat_name, fontweight='bold')\n",
        "        ax.set_ylabel('Count', fontweight='bold')\n",
        "        ax.set_title(f'{feat_name} Distribution', fontweight='bold')\n",
        "        ax.legend()\n",
        "        ax.grid(alpha=0.3, axis='y')\n",
        "\n",
        "    plt.suptitle('Feature Distributions: Correct vs Incorrect Predictions',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_distributions.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def analyze_low_confidence_predictions(y_true, y_pred, y_pred_probs, test_images,\n",
        "                                      test_features, feature_columns, threshold=0.7):\n",
        "    \"\"\"\n",
        "    Analyze predictions with low confidence (potential ambiguous cases).\n",
        "    \"\"\"\n",
        "    confidences = np.max(y_pred_probs, axis=1)\n",
        "    low_conf_idx = np.where(confidences < threshold)[0]\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"LOW CONFIDENCE PREDICTIONS (confidence < {threshold})\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Total low confidence predictions: {len(low_conf_idx)} ({len(low_conf_idx)/len(y_true)*100:.2f}%)\")\n",
        "\n",
        "    if len(low_conf_idx) > 0:\n",
        "        # Check how many are correct vs incorrect\n",
        "        low_conf_correct = np.sum(y_pred[low_conf_idx] == y_true[low_conf_idx])\n",
        "        low_conf_incorrect = len(low_conf_idx) - low_conf_correct\n",
        "\n",
        "        print(f\"  Correct: {low_conf_correct} ({low_conf_correct/len(low_conf_idx)*100:.1f}%)\")\n",
        "        print(f\"  Incorrect: {low_conf_incorrect} ({low_conf_incorrect/len(low_conf_idx)*100:.1f}%)\")\n",
        "        print()\n",
        "        print(\"These are ambiguous cases where the model is uncertain.\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# RUN ALL ANALYSES\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPREHENSIVE MISCLASSIFICATION ANALYSIS\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "# 1. Plot misclassified samples\n",
        "plot_misclassified_particles(y_true, y_pred, y_pred_probs, test_images,\n",
        "                             test_features, feature_columns, max_display=16)\n",
        "\n",
        "# 2. Confidence analysis\n",
        "print(\"\\n\")\n",
        "plot_confidence_distribution(y_true, y_pred, y_pred_probs)\n",
        "\n",
        "# 3. Feature distribution analysis\n",
        "print(\"\\n\")\n",
        "plot_feature_distributions_by_correctness(y_true, y_pred, test_features, feature_columns)\n",
        "\n",
        "# 4. Low confidence analysis\n",
        "print(\"\\n\")\n",
        "analyze_low_confidence_predictions(y_true, y_pred, y_pred_probs, test_images,\n",
        "                                  test_features, feature_columns, threshold=0.7)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Analysis complete! Check the saved images:\")\n",
        "print(\"  - misclassified_particles.png\")\n",
        "print(\"  - confidence_analysis.png\")\n",
        "print(\"  - feature_distributions.png\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoiW8loeGumi",
        "outputId": "6c1d353f-ae83-4ad0-a3bb-abb1abc62200"
      },
      "outputs": [],
      "source": [
        "# Make predictions on test set\n",
        "y_pred_probs = best_model.predict(test_ds, verbose=1)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(f\"Predictions shape: {y_pred_probs.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8zyh01NGumi"
      },
      "source": [
        "## 13. Generate Predictions and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWfXvLX0Gumi",
        "outputId": "a30bfa7a-8ac1-4aa3-85c4-b7c59bab9cab"
      },
      "outputs": [],
      "source": [
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\\n\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=['Liquid', 'Solid']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "yI0sq1XeGumi",
        "outputId": "fe66609f-d2f1-405f-fcd8-817baee582f2"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Liquid', 'Solid'])\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "plt.title('Confusion Matrix (Hybrid 2-Class CNN)')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nConfusion Matrix Analysis:\")\n",
        "for i, label in enumerate(['Liquid', 'Solid']):\n",
        "    correct = cm[i, i]\n",
        "    total = cm[i, :].sum()\n",
        "    print(f\"{label}: {correct}/{total} correct ({correct/total*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jao2sN3Gumi"
      },
      "source": [
        "## 14. Visualize Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcUg2nLIGumj"
      },
      "source": [
        "## 15. Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA8S_f-13Y08"
      },
      "outputs": [],
      "source": [
        "def create_image_only_model():\n",
        "    # Image input branch (CNN)\n",
        "    image_input = Input(shape=(128, 128, 1), name='image_input')\n",
        "\n",
        "    # Reduce initial filters for simpler images\n",
        "    x = Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "              kernel_regularizer=regularizers.l2(L2_REG))(image_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = SpatialDropout2D(0.2)(x)  # Spatial dropout for conv layers\n",
        "\n",
        "    x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "              kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = SpatialDropout2D(0.2)(x)\n",
        "\n",
        "    x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same',\n",
        "              kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = SpatialDropout2D(0.3)(x)\n",
        "\n",
        "    # Flatten the CNN output\n",
        "    x = Flatten()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    cnn_output = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "\n",
        "    # --- Final classification layers (Now connect directly to cnn_output) ---\n",
        "\n",
        "    z = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(cnn_output)\n",
        "    z = Dropout(0.5)(z)\n",
        "    z = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(z)\n",
        "    z = Dropout(0.3)(z)\n",
        "\n",
        "    # Output layer\n",
        "    output = Dense(2, activation='softmax', name='output')(z)\n",
        "\n",
        "    model = Model(inputs=image_input, outputs=output, name='Image_Only_CNN')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfBVb7NY3YId"
      },
      "outputs": [],
      "source": [
        "def create_feature_only_model(len_feature_columns):\n",
        "\n",
        "    # Environmental features input branch\n",
        "    feature_input = Input(shape=(len_feature_columns,), name='feature_input')\n",
        "\n",
        "    # Process environmental features (Identical to Hybrid Model)\n",
        "    f = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(feature_input)\n",
        "    f = Dropout(0.3)(f)\n",
        "    feature_output = Dense(16, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(f)\n",
        "\n",
        "    # --- Final classification layers (Now connect directly to feature_output) ---\n",
        "    z = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(feature_output)\n",
        "    z = Dropout(0.5)(z)\n",
        "    z = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(z)\n",
        "    z = Dropout(0.3)(z)\n",
        "\n",
        "    # Output layer\n",
        "    output = Dense(2, activation='softmax', name='output')(z)\n",
        "\n",
        "    model = Model(inputs=feature_input, outputs=output, name='Feature_Only_Dense')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI7O-1Qa6nMw"
      },
      "outputs": [],
      "source": [
        "def load_and_augment_image_only(\n",
        "    image_path: str, label: tf.Tensor, augment: bool\n",
        ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    \"\"\"\n",
        "    IMAGE-ONLY FUNCTION: Loads an image, preprocesses it, applies conditional augmentation,\n",
        "    and returns the structured input and label for the image-only model: (image, label).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Image Loading and Initial Processing\n",
        "    image = tf.io.read_file(filename=image_path)\n",
        "    image = tf.image.decode_png(contents=image, channels=1)\n",
        "    image = tf.image.convert_image_dtype(image=image, dtype=tf.float32)\n",
        "    image = tf.image.resize(images=image, size=IMAGE_SIZE)\n",
        "\n",
        "    # 2. Apply Augmentation (Only if 'augment' is True and based on probability)\n",
        "    if augment:\n",
        "        if tf.random.uniform(()) < AUGMENTATION_PROB:\n",
        "            # 1. Random horizontal flipping\n",
        "            image = tf.image.flip_left_right(image)\n",
        "        if tf.random.uniform(()) < AUGMENTATION_PROB:\n",
        "            # 2. Geometric augmentation (your custom function)\n",
        "            image = random_geometric_augment(image)\n",
        "\n",
        "    # 3. Final Normalization/Clipping\n",
        "    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n",
        "\n",
        "    # 4. Return in the format Keras model.fit expects: ( image_input, label )\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def create_feature_only_ds(features: tf.Tensor, labels: tf.Tensor) -> tf.data.Dataset:\n",
        "    \"\"\"\n",
        "    FEATURE-ONLY FUNCTION: Creates a dataset mapping only feature vectors and labels\n",
        "    directly from tensor slices.\n",
        "    \"\"\"\n",
        "    # Features are typically already preprocessed NumPy arrays (tensors)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "    # No map function is needed as no file loading or augmentation is performed\n",
        "    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acdnpNMq7GRs"
      },
      "outputs": [],
      "source": [
        "def create_image_only_ds(paths, labels, augment=False):\n",
        "    \"\"\"\n",
        "    Creates dataset mapping only image paths and labels, using the dedicated\n",
        "    load_and_augment_image_only function.\n",
        "    \"\"\"\n",
        "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
        "\n",
        "    # Assuming 'load_and_augment_image_only' is available in the environment\n",
        "    ds = ds.map(\n",
        "        lambda p, l: load_and_augment_image_only(p, l, augment=augment),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "def create_feature_only_ds(features, labels):\n",
        "    \"\"\"Creates dataset mapping only feature vectors and labels.\"\"\"\n",
        "    ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPrgKzLb4cfw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Load/Define the Models\n",
        "\n",
        "# Baseline Model (Best Hybrid Model)\n",
        "try:\n",
        "    baseline_model = load_model('best_particle_classifier_hybrid_binary.keras')\n",
        "except Exception as e:\n",
        "    print(f\"Error loading baseline model: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Image-Only Model (Need to define/import create_image_only_model)\n",
        "image_only_model = create_image_only_model()\n",
        "compile_model(image_only_model)\n",
        "\n",
        "# Feature-Only Model (Need to define/import create_feature_only_model)\n",
        "feature_only_model = create_feature_only_model(len(feature_columns))\n",
        "compile_model(feature_only_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CVydM60dwpP"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "  # Plot training history\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "  # Plot accuracy\n",
        "  axes[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
        "  axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "  axes[0].set_title('Model Accuracy (Hybrid CNN)')\n",
        "  axes[0].set_ylabel('Accuracy')\n",
        "  axes[0].set_xlabel('Epoch')\n",
        "  axes[0].legend()\n",
        "  axes[0].grid(True)\n",
        "\n",
        "  # Plot loss\n",
        "  axes[1].plot(history.history['loss'], label='Training Loss')\n",
        "  axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
        "  axes[1].set_title('Model Loss (Hybrid CNN)')\n",
        "  axes[1].set_ylabel('Loss')\n",
        "  axes[1].set_xlabel('Epoch')\n",
        "  axes[1].legend()\n",
        "  axes[1].grid(True)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  # Print final metrics\n",
        "  print(f\"\\nFinal Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
        "  print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
        "  print(f\"Best Validation Accuracy: {max(history.history['val_accuracy']):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcilOAcB3c-x",
        "outputId": "bb7d7447-7071-4cb0-d07c-0162bd3a0058"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"STARTING IMAGE-ONLY MODEL TRAINING\")\n",
        "print(\"=\" * 50)\n",
        "N_EPOCHS=40\n",
        "\n",
        "# Create datasets for image-only training\n",
        "train_ds_img = create_image_only_ds(X_paths_train, y_train, augment=True)\n",
        "val_ds_img = create_image_only_ds(X_paths_val, y_val, augment=False)\n",
        "\n",
        "steps_per_epoch_train = N_TRAIN_SAMPLES // BATCH_SIZE\n",
        "validation_steps_val = N_VAL_SAMPLES // BATCH_SIZE\n",
        "\n",
        "checkpoint_img = ModelCheckpoint('best_particle_classifier_image_only.keras', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "\n",
        "hist_image = image_only_model.fit(\n",
        "    train_ds_img,\n",
        "    steps_per_epoch=steps_per_epoch_train,\n",
        "    epochs=N_EPOCHS,\n",
        "    validation_data=val_ds_img,\n",
        "    validation_steps=validation_steps_val,\n",
        "    callbacks=[early_stop, checkpoint_img],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "SkNbj-3Jt8IQ",
        "outputId": "a67ee42e-dfdd-4a57-964e-55bffcf875d2"
      },
      "outputs": [],
      "source": [
        "plot_training_history(hist_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nggc_PHl5k8a",
        "outputId": "bc563508-e5c1-43bc-a3d7-cf4d6c2950b1"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"STARTING FEATURE-ONLY MODEL TRAINING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create datasets for feature-only training\n",
        "train_ds_feat = create_feature_only_ds(X_features_train, y_train)\n",
        "val_ds_feat = create_feature_only_ds(X_features_val, y_val)\n",
        "\n",
        "checkpoint_feat = ModelCheckpoint('best_particle_classifier_feature_only.keras', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "\n",
        "hist_feat = feature_only_model.fit(\n",
        "    train_ds_feat,\n",
        "    steps_per_epoch=steps_per_epoch_train, # Reuse step count for simplicity, assuming same size\n",
        "    epochs=N_EPOCHS,\n",
        "    validation_data=val_ds_feat,\n",
        "    validation_steps=validation_steps_val,\n",
        "    callbacks=[early_stop, checkpoint_feat],\n",
        "    verbose=1\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "iblthy_6ekKP",
        "outputId": "c2dbdde0-5c5a-47b0-f7b3-30b0d266aabf"
      },
      "outputs": [],
      "source": [
        "plot_training_history(hist_feat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_FwiZvqrg8n6",
        "outputId": "585acc0a-8e2d-4801-eb7d-e79af6f0d4f4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, Conv2D, MaxPooling2D, BatchNormalization, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Your configuration\n",
        "BATCH_SIZE = 16\n",
        "IMAGE_SIZE = (128, 128)\n",
        "L2_REG = 0.0001\n",
        "\n",
        "def build_cnn_branch():\n",
        "    \"\"\"Build the CNN branch of your hybrid model.\"\"\"\n",
        "    image_input = Input(shape=(128, 128, 1), name='image_input')\n",
        "\n",
        "    x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(image_input)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
        "\n",
        "    x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
        "\n",
        "    x = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
        "\n",
        "    x = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), padding='same')(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    cnn_output = Dense(128, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
        "\n",
        "    return Model(inputs=image_input, outputs=cnn_output, name='cnn_branch')\n",
        "\n",
        "\n",
        "def build_hybrid_model(num_features):\n",
        "    \"\"\"Build hybrid model with specified number of features.\"\"\"\n",
        "    # Image branch\n",
        "    image_input = Input(shape=(128, 128, 1), name='image_input')\n",
        "    cnn_branch = build_cnn_branch()\n",
        "    cnn_output = cnn_branch(image_input)\n",
        "\n",
        "    # Feature branch\n",
        "    feature_input = Input(shape=(num_features,), name='feature_input')\n",
        "    f = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(feature_input)\n",
        "    f = Dropout(0.3)(f)\n",
        "    feature_output = Dense(16, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(f)\n",
        "\n",
        "    # Combine\n",
        "    combined = Concatenate()([cnn_output, feature_output])\n",
        "\n",
        "    # Final layers\n",
        "    z = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(combined)\n",
        "    z = Dropout(0.5)(z)\n",
        "    z = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(L2_REG))(z)\n",
        "    z = Dropout(0.3)(z)\n",
        "\n",
        "    output = Dense(2, activation='softmax', name='output')(z)\n",
        "\n",
        "    model = Model(inputs=[image_input, feature_input], outputs=output, name='Hybrid_CNN')\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.00005),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_dataset_with_feature_subset(X_paths, X_features, y, feature_indices, augment=False):\n",
        "    \"\"\"Create tf.data.Dataset with only selected features.\"\"\"\n",
        "    # Select only the specified feature columns\n",
        "    X_features_subset = X_features[:, feature_indices]\n",
        "\n",
        "    # Create dataset\n",
        "    ds = tf.data.Dataset.from_tensor_slices((X_paths, X_features_subset, y))\n",
        "\n",
        "    # Map with augmentation\n",
        "    ds = ds.map(\n",
        "        lambda p, f, l: load_and_augment_hybrid(p, f, l, augment=augment),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    # Cache, shuffle (if training), batch, prefetch\n",
        "    ds = ds.cache()\n",
        "    if augment:  # Only shuffle training data\n",
        "        ds = ds.shuffle(buffer_size=len(X_paths))\n",
        "    ds = ds.batch(BATCH_SIZE)\n",
        "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "\n",
        "\n",
        "def feature_ablation_study(X_paths_train, X_features_train, y_train,\n",
        "                           X_paths_val, X_features_val, y_val,\n",
        "                           X_paths_test, X_features_test, y_test,\n",
        "                           feature_columns):\n",
        "    \"\"\"\n",
        "    Run comprehensive feature ablation study.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with results for each feature combination tested\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    n_features_total = len(feature_columns)\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"FEATURE ABLATION STUDY\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Total features: {n_features_total}\")\n",
        "    print(f\"Features: {feature_columns}\")\n",
        "    print()\n",
        "\n",
        "    # ==========================================\n",
        "    # 1. BASELINE: All features\n",
        "    # ==========================================\n",
        "    print(\"=\" * 80)\n",
        "    print(\"1. BASELINE - ALL FEATURES\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Use your existing datasets\n",
        "    model = build_hybrid_model(n_features_total)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        validation_data=val_ds,\n",
        "        epochs=30,\n",
        "        callbacks=[EarlyStopping(patience=10, restore_best_weights=True, verbose=1)],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
        "    baseline_acc = test_acc\n",
        "\n",
        "    results.append({\n",
        "        'experiment': 'ALL_FEATURES',\n",
        "        'n_features': n_features_total,\n",
        "        'features_used': ', '.join(feature_columns),\n",
        "        'test_accuracy': test_acc,\n",
        "        'test_loss': test_loss,\n",
        "        'accuracy_vs_baseline': 0.0\n",
        "    })\n",
        "\n",
        "    print(f\"✓ Baseline Test Accuracy: {test_acc:.4f}\\n\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 2. LEAVE-ONE-OUT: Remove each feature\n",
        "    # ==========================================\n",
        "    print(\"=\" * 80)\n",
        "    print(\"2. LEAVE-ONE-OUT ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Testing performance when each feature is removed...\\n\")\n",
        "\n",
        "    for i, feature_to_remove in enumerate(feature_columns):\n",
        "        print(f\"[{i+1}/{n_features_total}] Removing: {feature_to_remove}\")\n",
        "\n",
        "        # Get indices of features to keep\n",
        "        feature_indices = [j for j, f in enumerate(feature_columns) if f != feature_to_remove]\n",
        "        remaining_features = [f for f in feature_columns if f != feature_to_remove]\n",
        "\n",
        "        # Create datasets with subset of features\n",
        "        train_ds_subset = create_dataset_with_feature_subset(\n",
        "            X_paths_train, X_features_train, y_train, feature_indices, augment=True\n",
        "        )\n",
        "        val_ds_subset = create_dataset_with_feature_subset(\n",
        "            X_paths_val, X_features_val, y_val, feature_indices, augment=False\n",
        "        )\n",
        "        test_ds_subset = create_dataset_with_feature_subset(\n",
        "            X_paths_test, X_features_test, y_test, feature_indices, augment=False\n",
        "        )\n",
        "\n",
        "        # Build and train model\n",
        "        model = build_hybrid_model(len(remaining_features))\n",
        "\n",
        "        history = model.fit(\n",
        "            train_ds_subset,\n",
        "            validation_data=val_ds_subset,\n",
        "            epochs=30,\n",
        "            callbacks=[EarlyStopping(patience=10, restore_best_weights=True, verbose=0)],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        test_loss, test_acc = model.evaluate(test_ds_subset, verbose=0)\n",
        "        accuracy_drop = baseline_acc - test_acc\n",
        "\n",
        "        results.append({\n",
        "            'experiment': f'REMOVE_{feature_to_remove}',\n",
        "            'n_features': len(remaining_features),\n",
        "            'features_used': ', '.join(remaining_features),\n",
        "            'removed_feature': feature_to_remove,\n",
        "            'test_accuracy': test_acc,\n",
        "            'test_loss': test_loss,\n",
        "            'accuracy_vs_baseline': accuracy_drop\n",
        "        })\n",
        "\n",
        "        symbol = \"⚠️\" if accuracy_drop > 0.01 else \"✓\"\n",
        "        print(f\"  {symbol} Accuracy: {test_acc:.4f} (Δ = {accuracy_drop:+.4f})\\n\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 3. SINGLE FEATURE: Use only one feature\n",
        "    # ==========================================\n",
        "    print(\"=\" * 80)\n",
        "    print(\"3. SINGLE FEATURE ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Testing performance with each feature individually...\\n\")\n",
        "\n",
        "    for i, feature in enumerate(feature_columns):\n",
        "        print(f\"[{i+1}/{n_features_total}] Using only: {feature}\")\n",
        "\n",
        "        # Get index of this feature\n",
        "        for ind,feat in enumerate(feature_columns):\n",
        "            if feat == feature_name:\n",
        "                feature_idx = ind\n",
        "\n",
        "        # Create datasets with single feature\n",
        "        train_ds_single = create_dataset_with_feature_subset(\n",
        "            X_paths_train, X_features_train, y_train, feature_idx, augment=True\n",
        "        )\n",
        "        val_ds_single = create_dataset_with_feature_subset(\n",
        "            X_paths_val, X_features_val, y_val, feature_idx, augment=False\n",
        "        )\n",
        "        test_ds_single = create_dataset_with_feature_subset(\n",
        "            X_paths_test, X_features_test, y_test, feature_idx, augment=False\n",
        "        )\n",
        "\n",
        "        # Build and train model\n",
        "        model = build_hybrid_model(1)\n",
        "\n",
        "        history = model.fit(\n",
        "            train_ds_single,\n",
        "            validation_data=val_ds_single,\n",
        "            epochs=30,\n",
        "            callbacks=[EarlyStopping(patience=10, restore_best_weights=True, verbose=0)],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        test_loss, test_acc = model.evaluate(test_ds_single, verbose=0)\n",
        "\n",
        "        results.append({\n",
        "            'experiment': f'ONLY_{feature}',\n",
        "            'n_features': 1,\n",
        "            'features_used': feature,\n",
        "            'test_accuracy': test_acc,\n",
        "            'test_loss': test_loss,\n",
        "            'accuracy_vs_baseline': test_acc - baseline_acc\n",
        "        })\n",
        "\n",
        "        print(f\"  Accuracy: {test_acc:.4f} (vs baseline: {test_acc - baseline_acc:+.4f})\\n\")\n",
        "\n",
        "    # ==========================================\n",
        "    # Create results DataFrame\n",
        "    # ==========================================\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    return results_df, baseline_acc\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# RUN THE STUDY\n",
        "# ==========================================\n",
        "\n",
        "# Make sure you have feature_columns defined\n",
        "# feature_columns = ['temperature', 'pressure', 'altitude', etc.]\n",
        "\n",
        "results_df, baseline_acc = feature_ablation_study(\n",
        "    X_paths_train, X_features_train, y_train,\n",
        "    X_paths_val, X_features_val, y_val,\n",
        "    X_paths_test, X_features_test, y_test,\n",
        "    feature_columns\n",
        ")\n",
        "\n",
        "# Save results\n",
        "results_df.to_csv('feature_ablation_results.csv', index=False)\n",
        "print(\"\\n✓ Results saved to 'feature_ablation_results.csv'\")\n",
        "\n",
        "# ==========================================\n",
        "# ANALYSIS AND VISUALIZATION\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ABLATION STUDY RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Feature importance from leave-one-out\n",
        "print(\"\\n📊 FEATURE IMPORTANCE (Leave-One-Out)\")\n",
        "print(\"-\" * 80)\n",
        "leave_one_out = results_df[results_df['experiment'].str.startswith('REMOVE_')].copy()\n",
        "leave_one_out = leave_one_out.sort_values('accuracy_vs_baseline', ascending=False)\n",
        "\n",
        "print(\"\\nMost Important Features (largest accuracy drop when removed):\")\n",
        "print(leave_one_out[['removed_feature', 'test_accuracy', 'accuracy_vs_baseline']].to_string(index=False))\n",
        "\n",
        "# Single feature performance\n",
        "print(\"\\n📊 SINGLE FEATURE PERFORMANCE\")\n",
        "print(\"-\" * 80)\n",
        "single_features = results_df[results_df['experiment'].str.startswith('ONLY_')].copy()\n",
        "single_features = single_features.sort_values('test_accuracy', ascending=False)\n",
        "\n",
        "print(\"\\nBest Individual Features:\")\n",
        "print(single_features[['features_used', 'test_accuracy', 'accuracy_vs_baseline']].to_string(index=False))\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n📊 SUMMARY STATISTICS\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Baseline (all features): {baseline_acc:.4f}\")\n",
        "print(f\"Best single feature: {single_features.iloc[0]['features_used']} ({single_features.iloc[0]['test_accuracy']:.4f})\")\n",
        "print(f\"Worst single feature: {single_features.iloc[-1]['features_used']} ({single_features.iloc[-1]['test_accuracy']:.4f})\")\n",
        "print(f\"Most critical feature: {leave_one_out.iloc[0]['removed_feature']} (drops {leave_one_out.iloc[0]['accuracy_vs_baseline']:.4f})\")\n",
        "print(f\"Least critical feature: {leave_one_out.iloc[-1]['removed_feature']} (drops {leave_one_out.iloc[-1]['accuracy_vs_baseline']:.4f})\")\n",
        "\n",
        "# ==========================================\n",
        "# VISUALIZATIONS\n",
        "# ==========================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Leave-one-out importance\n",
        "ax1 = axes[0, 0]\n",
        "leave_one_out_plot = leave_one_out.sort_values('accuracy_vs_baseline')\n",
        "colors = ['red' if x > 0.01 else 'steelblue' for x in leave_one_out_plot['accuracy_vs_baseline']]\n",
        "ax1.barh(leave_one_out_plot['removed_feature'], leave_one_out_plot['accuracy_vs_baseline'], color=colors)\n",
        "ax1.set_xlabel('Accuracy Drop When Removed', fontweight='bold')\n",
        "ax1.set_title('Feature Importance (Leave-One-Out)\\nRed = Critical Features', fontweight='bold', fontsize=12)\n",
        "ax1.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
        "ax1.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Plot 2: Single feature performance\n",
        "ax2 = axes[0, 1]\n",
        "single_features_plot = single_features.sort_values('test_accuracy')\n",
        "ax2.barh(single_features_plot['features_used'], single_features_plot['test_accuracy'], color='skyblue')\n",
        "ax2.axvline(x=baseline_acc, color='red', linestyle='--', linewidth=2, label=f'All Features ({baseline_acc:.3f})')\n",
        "ax2.set_xlabel('Test Accuracy', fontweight='bold')\n",
        "ax2.set_title('Individual Feature Performance\\n(Image + Single Feature)', fontweight='bold', fontsize=12)\n",
        "ax2.legend()\n",
        "ax2.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Plot 3: Accuracy vs baseline comparison\n",
        "ax3 = axes[1, 0]\n",
        "all_experiments = results_df[results_df['experiment'] != 'ALL_FEATURES'].copy()\n",
        "all_experiments = all_experiments.sort_values('test_accuracy', ascending=False).head(10)\n",
        "colors_acc = ['green' if x >= baseline_acc else 'orange' for x in all_experiments['test_accuracy']]\n",
        "ax3.barh(range(len(all_experiments)), all_experiments['test_accuracy'], color=colors_acc)\n",
        "ax3.set_yticks(range(len(all_experiments)))\n",
        "ax3.set_yticklabels([exp.replace('ONLY_', '').replace('REMOVE_', 'No ') for exp in all_experiments['experiment']], fontsize=9)\n",
        "ax3.axvline(x=baseline_acc, color='red', linestyle='--', linewidth=2, label='Baseline')\n",
        "ax3.set_xlabel('Test Accuracy', fontweight='bold')\n",
        "ax3.set_title('Top 10 Feature Combinations', fontweight='bold', fontsize=12)\n",
        "ax3.legend()\n",
        "ax3.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Plot 4: Feature correlation heatmap (if you want to add it)\n",
        "ax4 = axes[1, 1]\n",
        "# Calculate correlation between accuracy drop and single feature performance\n",
        "feature_comparison = leave_one_out.merge(\n",
        "    single_features[['features_used', 'test_accuracy']],\n",
        "    left_on='removed_feature',\n",
        "    right_on='features_used',\n",
        "    suffixes=('_removed', '_single')\n",
        ")\n",
        "if len(feature_comparison) > 0:\n",
        "    ax4.scatter(feature_comparison['accuracy_vs_baseline'],\n",
        "               feature_comparison['test_accuracy_single'],\n",
        "               s=100, alpha=0.6, color='purple')\n",
        "    for idx, row in feature_comparison.iterrows():\n",
        "        ax4.annotate(row['removed_feature'],\n",
        "                    (row['accuracy_vs_baseline'], row['test_accuracy_single']),\n",
        "                    fontsize=8, alpha=0.7)\n",
        "    ax4.set_xlabel('Accuracy Drop When Removed', fontweight='bold')\n",
        "    ax4.set_ylabel('Accuracy as Single Feature', fontweight='bold')\n",
        "    ax4.set_title('Feature Importance: Removal vs Individual Performance', fontweight='bold', fontsize=12)\n",
        "    ax4.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_ablation_analysis.png', dpi=300, bbox_inches='tight')\n",
        "print(\"\\n✓ Visualization saved to 'feature_ablation_analysis.png'\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "Ri_P2nHml4Hx",
        "outputId": "f4286c25-7e36-480b-cf26-f9d7ea2c1b5b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create DataFrame of your features\n",
        "feature_df = pd.DataFrame(\n",
        "    X_features_train,\n",
        "    columns=feature_columns\n",
        ")\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = feature_df.corr()\n",
        "\n",
        "print(\"Feature Correlation Matrix:\")\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Feature Correlation Matrix', fontweight='bold', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_correlation.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Check for high correlations\n",
        "print(\"\\nHighly Correlated Feature Pairs (|r| > 0.8):\")\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        corr = correlation_matrix.iloc[i, j]\n",
        "        if abs(corr) > 0.8:\n",
        "            print(f\"{correlation_matrix.columns[i]} <-> {correlation_matrix.columns[j]}: {corr:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92T6d3in5sU9",
        "outputId": "1740308b-2ca7-47f3-82ed-6c1dcce07c45"
      },
      "outputs": [],
      "source": [
        "# Load the best weights for ablation\n",
        "best_image_only = load_model('best_particle_classifier_image_only.keras')\n",
        "best_feature_only = load_model('best_particle_classifier_feature_only.keras')\n",
        "\n",
        "# 3. Evaluate All Models on Test Set\n",
        "\n",
        "# Prepare test datasets for ablated models\n",
        "test_ds_img = create_image_only_ds(X_paths_test, y_test, augment=False)\n",
        "test_ds_feat = create_feature_only_ds(X_features_test, y_test)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, ds):\n",
        "    metrics = model.evaluate(ds, verbose=0)\n",
        "    return metrics[4] # Return AUC\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"BRANCH ABLATION RESULTS (TEST AUC)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Calculate AUC for all three models\n",
        "auc_baseline = evaluate_model(baseline_model, test_ds)\n",
        "auc_img_only = evaluate_model(best_image_only, test_ds_img)\n",
        "auc_feat_only = evaluate_model(best_feature_only, test_ds_feat)\n",
        "\n",
        "results = {\n",
        "    'Hybrid Model': auc_baseline,\n",
        "    'Image-Only Model': auc_img_only,\n",
        "    'Feature-Only Model': auc_feat_only\n",
        "}\n",
        "\n",
        "print(f\"Hybrid Model (Baseline): {auc_baseline:.4f}\")\n",
        "print(f\"Image-Only Model:        {auc_img_only:.4f} (Drop: {auc_baseline - auc_img_only:.4f})\")\n",
        "print(f\"Feature-Only Model:      {auc_feat_only:.4f} (Drop: {auc_baseline - auc_feat_only:.4f})\")\n",
        "\n",
        "\n",
        "# 4. Individual Feature Ablation (Perturbing Test Data)\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"INDIVIDUAL FEATURE ABLATION (Hybrid Model Test AUC)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# We evaluate the performance of the full hybrid model when individual features are zeroed out.\n",
        "auc_individual_ablation = {}\n",
        "X_feat_test_original = X_features_test.copy()\n",
        "\n",
        "for i, feature_name in enumerate(feature_columns):\n",
        "\n",
        "    # 1. Create a perturbed test feature set (zero out column i)\n",
        "    X_feat_test_perturbed = X_feat_test_original.copy()\n",
        "    X_feat_test_perturbed[:, i] = 0.0 # Set column i to zero\n",
        "\n",
        "    # 2. Create the perturbed hybrid test dataset\n",
        "    test_ds_perturbed = tf.data.Dataset.from_tensor_slices(\n",
        "        (X_paths_test, X_feat_test_perturbed, y_test)\n",
        "    )\n",
        "    test_ds_perturbed = test_ds_perturbed.map(\n",
        "        lambda p, f, l: load_and_augment_hybrid(p, f, l, augment=False),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    # 3. Evaluate the full hybrid model\n",
        "    metrics = baseline_model.evaluate(test_ds_perturbed, verbose=0)\n",
        "    auc_perturbed = metrics[4]\n",
        "\n",
        "    drop = auc_baseline - auc_perturbed\n",
        "    auc_individual_ablation[feature_name] = {'AUC': auc_perturbed, 'Drop': drop}\n",
        "\n",
        "# Display ranked results\n",
        "ranked_results = sorted(auc_individual_ablation.items(), key=lambda item: item[1]['Drop'], reverse=True)\n",
        "\n",
        "print(\"Feature\\t\\tAUC After Ablation\\tAUC Drop\")\n",
        "print(\"-\" * 50)\n",
        "for feature, data in ranked_results:\n",
        "    print(f\"{feature}:\\t\\t{data['AUC']:.4f}\\t\\t\\t{data['Drop']:.4f}\")\n",
        "\n",
        "print(\"\\nFinal Branch Ablation Summary:\")\n",
        "print(\"A higher AUC Drop indicates a more important branch/feature.\")\n",
        "\n",
        "\n",
        "# Example usage (uncomment and run if all data variables are defined)\n",
        "# run_ablation_analysis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29nZSAbZ9w2_"
      },
      "outputs": [],
      "source": [
        "##Ablation analysis:\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Data provided by the user's ablation analysis ---\n",
        "\n",
        "# 1. Branch Ablation Results\n",
        "BRANCH_RESULTS = {\n",
        "    'Model': ['Hybrid (Baseline)', 'Image-Only', 'Feature-Only'],\n",
        "    'AUC': [0.9954, 0.9791, 1.0000],\n",
        "    'Drop': [0.0000, 0.0163, -0.0046] # Negative drop means performance improved\n",
        "}\n",
        "\n",
        "# 2. Individual Feature Ablation Results\n",
        "FEATURE_RESULTS = {\n",
        "    'Feature': ['aircrafttas', 'diam', 'xsize', 'ysize', 'area', 'GGALT', 'WSC', 'WDC', 'PALT_A'],\n",
        "    'Drop': [0.0461, 0.0023, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
        "}\n",
        "\n",
        "def plot_branch_ablation(branch_df: pd.DataFrame):\n",
        "    \"\"\"Generates a bar chart comparing the AUC of the three main models.\"\"\"\n",
        "    plt.figure(figsize=(9, 6))\n",
        "    sns.barplot(x='Model', y='AUC', data=branch_df, palette='viridis')\n",
        "\n",
        "    # Add AUC values on top of bars\n",
        "    for index, row in branch_df.iterrows():\n",
        "        plt.text(index, row['AUC'] - 0.005, f\"{row['AUC']:.4f}\", color='white', ha='center', fontweight='bold')\n",
        "\n",
        "    plt.ylim(min(branch_df['AUC']) - 0.01, 1.01)\n",
        "    plt.title('Branch Ablation Analysis: Model Performance (Test AUC)', fontsize=16)\n",
        "    plt.ylabel('Test AUC Score', fontsize=12)\n",
        "    plt.xlabel('Model Configuration', fontsize=12)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_feature_importance(feature_df: pd.DataFrame):\n",
        "    \"\"\"Generates a ranked bar chart showing the AUC drop per feature ablation.\"\"\"\n",
        "\n",
        "    # Calculate performance change as a percentage drop for better interpretation\n",
        "    feature_df['Drop_Pct'] = (feature_df['Drop'] / BRANCH_RESULTS['AUC'][0]) * 100\n",
        "\n",
        "    # Sort by drop percentage\n",
        "    feature_df = feature_df.sort_values(by='Drop', ascending=False)\n",
        "\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    sns.barplot(x='Drop_Pct', y='Feature', data=feature_df, palette='magma')\n",
        "\n",
        "    # Add drop values\n",
        "    for index, row in feature_df.iterrows():\n",
        "        plt.text(row['Drop_Pct'] + 0.1, index, f\"{row['Drop_Pct']:.2f}% ({row['Drop']:.4f} AUC Drop)\",\n",
        "                 color='black', va='center')\n",
        "\n",
        "    plt.title('Individual Feature Importance: AUC Drop from Baseline', fontsize=16)\n",
        "    plt.xlabel('Performance Drop (%)', fontsize=12)\n",
        "    plt.ylabel('Ablated Feature', fontsize=12)\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_feature_separability_check(\n",
        "    X_feat_test: np.ndarray, y_test: np.ndarray, feature_columns: list, feature_name: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Plots the density of a single critical feature (e.g., 'aircrafttas')\n",
        "    separated by the true class label (0 or 1).\n",
        "\n",
        "    NOTE: X_feat_test and y_test must be provided from your main environment.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        for ind,feat in enumerate(feature_columns):\n",
        "            if feat == feature_name:\n",
        "                feature_index = ind\n",
        "    except ValueError:\n",
        "        print(f\"Error: Feature '{feature_name}' not found in feature_columns.\")\n",
        "        return\n",
        "\n",
        "    # Create a DataFrame for easy plotting\n",
        "    plot_df = pd.DataFrame({\n",
        "        feature_name: X_feat_test[:, feature_index],\n",
        "        'Class Label': np.argmax(y_test, axis=1) # Convert one-hot to class index\n",
        "    })\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Use Kernel Density Estimate (KDE) plot to show distribution overlap\n",
        "    sns.kdeplot(\n",
        "        data=plot_df,\n",
        "        x=feature_name,\n",
        "        hue='Class Label',\n",
        "        fill=True,\n",
        "        alpha=.5,\n",
        "        linewidth=2,\n",
        "        legend=True\n",
        "    )\n",
        "\n",
        "    plt.title(f'Feature Separability: Density of \"{feature_name}\" by Class', fontsize=16)\n",
        "    plt.xlabel(feature_name, fontsize=12)\n",
        "    plt.ylabel('Density', fontsize=12)\n",
        "    plt.legend(title='Class')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "PcnDEDzM92M5",
        "outputId": "355147da-9f37-4884-aa96-1b2e1248807e"
      },
      "outputs": [],
      "source": [
        "branch_df = pd.DataFrame(BRANCH_RESULTS)\n",
        "feature_df = pd.DataFrame(FEATURE_RESULTS)\n",
        "\n",
        "# 1. Visualize Branch Ablation\n",
        "plot_branch_ablation(branch_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "iR_X0i5A99vA",
        "outputId": "7b8403dc-507f-497d-d17f-0c4e9b690b21"
      },
      "outputs": [],
      "source": [
        "# 2. Visualize Individual Feature Importance\n",
        "plot_feature_importance(feature_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRSKCtaIH4lA",
        "outputId": "2a7f8ab1-09c9-4a59-8b7e-f44b8c0170e2"
      },
      "outputs": [],
      "source": [
        "feature_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "p2-cwRUF-CI9",
        "outputId": "37fde4ed-2b09-41b2-89ad-d9358fbf9ce2"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    plot_feature_separability_check(X_features_test, y_test, feature_columns, 'xsize')\n",
        "except NameError:\n",
        "    print(\"Skipping Feature Separability Check: Data arrays are not defined in this scope.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP4BZNxwAVQo"
      },
      "outputs": [],
      "source": [
        "def find_optimal_threshold(\n",
        "    X_feat_test: np.ndarray, y_test: np.ndarray, feature_columns: list, feature_name: str\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"\n",
        "    Finds the optimal binary classification threshold for a single feature\n",
        "    by maximizing accuracy on the test set.\n",
        "\n",
        "    Args:\n",
        "        X_feat_test: NumPy array of test features (already normalized/scaled).\n",
        "        y_test: NumPy array of test labels (one-hot encoded).\n",
        "        feature_columns: List of feature names.\n",
        "        feature_name: The specific feature to analyze (e.g., 'aircrafttas').\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing (optimal_threshold, max_accuracy, corresponding_auc).\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        for ind,feat in enumerate(feature_columns):\n",
        "            if feat == feature_name:\n",
        "                feature_index = ind\n",
        "    except ValueError:\n",
        "        print(f\"Error: Feature '{feature_name}' not found in feature_columns.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # 1. Extract the feature vector and true binary labels\n",
        "    feature_data = X_feat_test[:, feature_index]\n",
        "    # Assuming the first column is the positive class (Class 1) or converting to 0/1 index\n",
        "    true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # 2. Get unique values to check as potential thresholds (sorted)\n",
        "    threshold_candidates = np.sort(np.unique(feature_data))\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    optimal_threshold = None\n",
        "\n",
        "    # 3. Iterate through every unique value as a potential threshold\n",
        "    for threshold in threshold_candidates:\n",
        "        # Predict: Class 1 if feature value > threshold, Class 0 otherwise.\n",
        "        # This assumes Class 1 is associated with higher values, based on the plot.\n",
        "        predictions = (feature_data > threshold).astype(int)\n",
        "\n",
        "        # Calculate accuracy for this threshold\n",
        "        accuracy = np.mean(predictions == true_labels)\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            optimal_threshold = threshold\n",
        "\n",
        "    # Calculate AUC for the final optimal threshold (optional, but good for completeness)\n",
        "    # Since we are using a single threshold and the data is perfectly separated,\n",
        "    # the AUC should match the reported 1.0000.\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    final_predictions_proba = feature_data # Use feature value as the score\n",
        "    try:\n",
        "        final_auc = roc_auc_score(true_labels, final_predictions_proba)\n",
        "    except:\n",
        "        final_auc = 1.0 # If AUC is exactly 1.0, roc_auc_score might throw a warning/error on perfect separation\n",
        "\n",
        "    return optimal_threshold, best_accuracy, final_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcnmGcCyAWcw",
        "outputId": "71e1f039-83df-4675-b5b2-a2b7552fb762"
      },
      "outputs": [],
      "source": [
        "threshold, accuracy, auc = find_optimal_threshold(\n",
        "  X_features_test, y_test, feature_columns, 'aircrafttas'\n",
        ")\n",
        "\n",
        "if threshold is not None:\n",
        "  print(f\"\\nOptimal Threshold Analysis for 'aircrafttas':\")\n",
        "  print(\"-\" * 40)\n",
        "  print(f\"Optimal Threshold Value: {threshold:.4f}\")\n",
        "  print(f\"Classification Accuracy at this Threshold: {accuracy:.4f}\")\n",
        "  print(f\"Corresponding AUC (Score based on raw feature): {auc:.4f}\")\n",
        "  print(\"\\nInterpretation:\")\n",
        "  print(f\"If aircrafttas > {threshold:.4f}, predict Class 1.\")\n",
        "  print(f\"If aircrafttas <= {threshold:.4f}, predict Class 0.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "21X6fQMdA9jI",
        "outputId": "99e7673d-4c05-4f82-b62f-da9b74a2c721"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=df_valid['Time'], y=df_valid['aircrafttas'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYIHfJYqGKqw",
        "outputId": "80b4b091-7a43-4dcd-975e-a32f959ed8ff"
      },
      "outputs": [],
      "source": [
        "len(df_valid)/10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "KboU75_OHIBP",
        "outputId": "66e9efc2-d9eb-4b40-89a5-4e85eeaf092d"
      },
      "outputs": [],
      "source": [
        "df_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsXMjb2VGDDR"
      },
      "outputs": [],
      "source": [
        "xticks = []\n",
        "for x in range(0,len(df_valid),433):\n",
        "  xticks.append(df_valid['Time'].iloc[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O6GsTQykFkYS",
        "outputId": "cf66b585-5f34-4866-8af4-5ec7834b5d6a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "for feature in feature_columns:\n",
        "  sns.scatterplot(x=df_valid['Time'], y=df_valid[feature], hue=df_valid['phase'])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "oFzHn6qjBv4X",
        "outputId": "8b925982-0895-4281-883e-a251d1a5544b"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=df_valid['Time'], y=df_valid['diam'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUtBS64OBvtb"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jK6cRtfBGumj"
      },
      "source": [
        "## 16. Save Predictions (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGLEGH4DGumj"
      },
      "outputs": [],
      "source": [
        "# Save predictions to CSV for further analysis\n",
        "# predictions_df = pd.DataFrame({\n",
        "#     'particle_idx_seq': df_valid.iloc[test_indices]['particle_idx_seq'].values,\n",
        "#     'true_label': y_true,\n",
        "#     'predicted_label': y_pred,\n",
        "#     'liquid_probability': y_pred_probs[:, 0],\n",
        "#     'solid_probability': y_pred_probs[:, 1],\n",
        "#     'donut_probability': y_pred_probs[:, 2],\n",
        "#     'noise_probability': y_pred_probs[:, 3],\n",
        "#     'temperature': X_feat_test[:, 0] * scaler.scale_[0] + scaler.mean_[0],\n",
        "#     'air_speed': X_feat_test[:, 1] * scaler.scale_[1] + scaler.mean_[1],\n",
        "#     'altitude': X_feat_test[:, 2] * scaler.scale_[2] + scaler.mean_[2]\n",
        "# })\n",
        "# predictions_df.to_csv('particle_predictions_hybrid_4class.csv', index=False)\n",
        "# print(\"Predictions saved to 'particle_predictions_hybrid_4class.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TISvzAD5Gumj"
      },
      "source": [
        "## Notes and Next Steps\n",
        "\n",
        "### Hybrid 4-Class Model Architecture:\n",
        "- **CNN branch**: 4 convolutional blocks extract spatial/morphological features from particle images\n",
        "- **Environmental branch**: Dense layers process temperature, air speed, and altitude\n",
        "- **Fusion**: Both branches are concatenated before final classification\n",
        "- **Output**: 4 classes (Liquid, Solid, Donut, Noise)\n",
        "- **Regularization**: Dropout and BatchNormalization prevent overfitting\n",
        "- **Class weights**: Applied to handle class imbalance\n",
        "\n",
        "### Phase Descriptions:\n",
        "- **Liquid (0)**: Liquid water droplets\n",
        "- **Solid (1)**: Ice crystals\n",
        "- **Donut (2)**: Donut-shaped artifacts or special particle types\n",
        "- **Noise (3)**: Noisy or invalid particle images\n",
        "\n",
        "### Advantages of Hybrid Approach:\n",
        "1. **Multi-modal learning**: Combines visual and environmental information\n",
        "2. **Physical constraints**: Temperature can help disambiguate liquid vs ice\n",
        "3. **Context awareness**: Altitude and air speed provide atmospheric context\n",
        "4. **Improved accuracy**: Environmental features can improve classification, especially for ambiguous cases\n",
        "\n",
        "### Comparison with Image-Only Model:\n",
        "Compare this hybrid model with the image-only baseline:\n",
        "1. **Accuracy improvement**: How much do environmental features help overall?\n",
        "2. **Per-class performance**: Which phases benefit most from environmental data?\n",
        "3. **Confidence**: Does the hybrid model have higher prediction confidence?\n",
        "4. **Misclassification patterns**: Does it reduce specific confusion pairs?\n",
        "5. **Physical consistency**: Are predictions more physically plausible?\n",
        "\n",
        "### Potential Improvements:\n",
        "1. **Data augmentation**: Add rotation, flipping, scaling to particle images\n",
        "2. **Attention mechanisms**: Let the model learn which features matter most\n",
        "3. **Different architectures**: Try ResNet, EfficientNet, or Vision Transformers\n",
        "4. **Hyperparameter tuning**: Learning rate, batch size, dropout rates, architecture depth\n",
        "5. **Ensemble methods**: Combine multiple models\n",
        "6. **Feature engineering**: Add derived features (e.g., supersaturation, distance from freezing)\n",
        "7. **Focal loss**: Better handling of hard examples\n",
        "8. **Cross-validation**: K-fold CV for more robust evaluation\n",
        "\n",
        "### Key Research Questions:\n",
        "- How much does temperature improve liquid/solid classification?\n",
        "- Are donut particles associated with specific atmospheric conditions?\n",
        "- Can environmental features help filter noise automatically?\n",
        "- What's the optimal balance between image and environmental features?\n",
        "- How well does the model generalize across different flight conditions?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
